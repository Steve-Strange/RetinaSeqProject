{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T10:17:27.187106Z",
     "iopub.status.busy": "2025-12-26T10:17:27.186304Z",
     "iopub.status.idle": "2025-12-26T10:17:38.866947Z",
     "shell.execute_reply": "2025-12-26T10:17:38.866281Z",
     "shell.execute_reply.started": "2025-12-26T10:17:27.187068Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1\n",
      "Train: 20 - 20 - 20\n",
      "Test: 20 - 20 - 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  8.80it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 50.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# Data Augmentation\n",
    "import sys\n",
    "import os\n",
    "\n",
    "if os.path.exists(\"config.py\"):\n",
    "    print(\"Warning: Found 'config.py' in the current directory. Please rename it to avoid conflicts with torch.\")\n",
    "if os.path.exists(\"torch.py\"):\n",
    "    print(\"Warning: Found 'torch.py' in the current directory. Please rename it to avoid conflicts with torch.\")\n",
    "\n",
    "# 优先导入 torch\n",
    "import torch\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import imageio\n",
    "from albumentations import HorizontalFlip, VerticalFlip, Rotate, RandomBrightnessContrast, RandomCrop, RandomRotate90, RandomGridShuffle\n",
    "\n",
    "\"\"\" Create a directory \"\"\"\n",
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "'''加载数据：原图+标签'''\n",
    "def load_data(path):\n",
    "    train_x = sorted(glob(os.path.join(path, \"training\", \"images\", \"*.tif\")))\n",
    "    train_y = sorted(glob(os.path.join(path, \"training\", \"1st_manual\", \"*.gif\")))\n",
    "\n",
    "    test_x = sorted(glob(os.path.join(path, \"test\", \"images\", \"*.tif\")))\n",
    "    test_y = sorted(glob(os.path.join(path, \"test\", \"1st_manual\", \"*.gif\")))\n",
    "\n",
    "    return (train_x, train_y), (test_x, test_y)\n",
    "\n",
    "'''\n",
    "增强数据\n",
    "对图像及其对应mask数据增强\n",
    "'''\n",
    "def load_data_with_od(path):\n",
    "    # 加载原图、血管Mask、视盘Mask\n",
    "    train_x = sorted(glob(os.path.join(path, \"training\", \"images\", \"*.tif\")))\n",
    "    train_y = sorted(glob(os.path.join(path, \"training\", \"1st_manual\", \"*.gif\")))\n",
    "    train_od = sorted(glob(os.path.join(path, \"training\", \"od_masks\", \"*.png\"))) # 新增\n",
    "\n",
    "    test_x = sorted(glob(os.path.join(path, \"test\", \"images\", \"*.tif\")))\n",
    "    test_y = sorted(glob(os.path.join(path, \"test\", \"1st_manual\", \"*.gif\")))\n",
    "    test_od = sorted(glob(os.path.join(path, \"test\", \"od_masks\", \"*.png\"))) # 新增\n",
    "\n",
    "    return (train_x, train_y, train_od), (test_x, test_y, test_od)\n",
    "\n",
    "def augment_data_triple_triple(images, masks, od_masks, save_path, augment=True):\n",
    "    size = (560, 560)\n",
    "    \n",
    "    # 确保保存路径存在\n",
    "    create_dir(os.path.join(save_path, \"image\"))\n",
    "    create_dir(os.path.join(save_path, \"mask\"))\n",
    "    create_dir(os.path.join(save_path, \"od_mask\")) # 新增\n",
    "\n",
    "    for idx, (x, y, od) in tqdm(enumerate(zip(images, masks, od_masks)), total=len(images)):\n",
    "        name = x.split(os.sep)[-1].split(\".\")[0]\n",
    "\n",
    "        x = cv2.imread(x, cv2.IMREAD_COLOR)\n",
    "        y = imageio.mimread(y)[0]\n",
    "        od = cv2.imread(od, cv2.IMREAD_GRAYSCALE) # 读取视盘Mask\n",
    "\n",
    "        if augment:\n",
    "            transformations = [\n",
    "                HorizontalFlip(p=1.0),\n",
    "                VerticalFlip(p=1.0),\n",
    "                Rotate(limit=45, p=1.0),\n",
    "                RandomBrightnessContrast(p=0.6),\n",
    "                RandomCrop(300, 300, p=0.8),\n",
    "                RandomRotate90(p=1.0),\n",
    "                RandomGridShuffle(p=0.7)\n",
    "            ]\n",
    "            \n",
    "            # 使用列表保存增强结果\n",
    "            X, Y, OD = [x], [y], [od]\n",
    "\n",
    "            for aug in transformations:\n",
    "                # Albumentations 支持多 Mask: use masks=[mask1, mask2]\n",
    "                augmented = aug(image=x, masks=[y, od]) \n",
    "                X.append(augmented[\"image\"])\n",
    "                Y.append(augmented[\"masks\"][0])\n",
    "                OD.append(augmented[\"masks\"][1])\n",
    "        else:\n",
    "            X, Y, OD = [x], [y], [od]\n",
    "\n",
    "        index = 0\n",
    "        for i, m, o in zip(X, Y, OD):\n",
    "            i = cv2.resize(i, size)\n",
    "            m = cv2.resize(m, size)\n",
    "            o = cv2.resize(o, size)\n",
    "\n",
    "            tmp_name = f\"{name}_{index}.png\"\n",
    "            \n",
    "            cv2.imwrite(os.path.join(save_path, \"image\", tmp_name), i)\n",
    "            cv2.imwrite(os.path.join(save_path, \"mask\", tmp_name), m)\n",
    "            cv2.imwrite(os.path.join(save_path, \"od_mask\", tmp_name), o) # 保存视盘Mask\n",
    "            index += 1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\" Seeding \"\"\"\n",
    "    np.random.seed(42)\n",
    "\n",
    "    \"\"\" Load the data \"\"\"\n",
    "    # 修改为相对路径\n",
    "    data_path = \"data/DRIVE/\"\n",
    "    \n",
    "    if os.path.exists(data_path):\n",
    "        (train_x, train_y, train_od), (test_x, test_y, test_od) = load_data_with_od(data_path)\n",
    "\n",
    "        print(f\"Train: {len(train_x)} - {len(train_y)} - {len(train_od)}\")\n",
    "        print(f\"Test: {len(test_x)} - {len(test_y)} - {len(test_od)}\")\n",
    "\n",
    "        \"\"\" Create directories to save the augmented data \"\"\"\n",
    "        create_dir(\"working/new_data/train/image/\")\n",
    "        create_dir(\"working/new_data/train/mask/\")\n",
    "        create_dir(\"working/new_data/test/image/\")\n",
    "        create_dir(\"working/new_data/test/mask/\")\n",
    "\n",
    "        \"\"\" Data augmentation \"\"\"\n",
    "        # 取消注释以运行数据增强\n",
    "        augment_data_triple_triple(train_x, train_y, train_od, \"working/new_data/train/\", augment=True)\n",
    "        augment_data_triple_triple(test_x, test_y, test_od, \"working/new_data/test/\", augment=False)\n",
    "    else:\n",
    "        print(f\"Data path not found: {data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T10:17:38.868551Z",
     "iopub.status.busy": "2025-12-26T10:17:38.868200Z",
     "iopub.status.idle": "2025-12-26T10:17:40.435461Z",
     "shell.execute_reply": "2025-12-26T10:17:40.434747Z",
     "shell.execute_reply.started": "2025-12-26T10:17:38.868526Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 560, 560])\n"
     ]
    }
   ],
   "source": [
    "# Model (LFA-Net)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalModulation(nn.Module):\n",
    "    def __init__(self, in_channels, gamma=2.0, alpha=0.25):\n",
    "        super(FocalModulation, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.gmp = nn.AdaptiveMaxPool2d(1)\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = self.gap(x)\n",
    "        max_val = self.gmp(x)\n",
    "        modulation = (max_val - mean) * self.alpha\n",
    "        modulation = self.conv(modulation)\n",
    "        modulation = self.sigmoid(modulation)\n",
    "        scaled_inputs = x * modulation\n",
    "        outputs = torch.pow(scaled_inputs, self.gamma)\n",
    "        return outputs\n",
    "\n",
    "class FocalModulationContextAggregation(nn.Module):\n",
    "    def __init__(self, in_channels, filters):\n",
    "        super(FocalModulationContextAggregation, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, filters, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(in_channels, filters, kernel_size=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv_ctx = nn.Conv2d(filters, filters, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.focal_mod = FocalModulation(filters)\n",
    "\n",
    "    def forward(self, x):\n",
    "        c1 = self.relu1(self.conv1(x))\n",
    "        c2 = self.relu2(self.conv2(x))\n",
    "        \n",
    "        global_context = self.gap(c2)\n",
    "        global_context = self.sigmoid(self.conv_ctx(global_context))\n",
    "        global_context = c1 * global_context\n",
    "        \n",
    "        fm = self.focal_mod(global_context)\n",
    "        return torch.cat([c1, fm], dim=1)\n",
    "\n",
    "class VisionMambaInspired(nn.Module):\n",
    "    def __init__(self, dim, dropout_rate=0.1):\n",
    "        super(VisionMambaInspired, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.token_mixer = nn.Sequential(\n",
    "            nn.Conv2d(dim, dim, kernel_size=3, padding=1, groups=dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.channel_mixer = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(dim * 4, dim),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        \n",
    "        shortcut = x\n",
    "        x_perm = x.permute(0, 2, 3, 1)\n",
    "        x_norm = self.norm1(x_perm).permute(0, 3, 1, 2)\n",
    "        x_tm = self.token_mixer(x_norm) + shortcut\n",
    "        \n",
    "        shortcut = x_tm\n",
    "        x_perm = x_tm.permute(0, 2, 3, 1)\n",
    "        x_norm = self.norm2(x_perm)\n",
    "        x_cm = self.channel_mixer(x_norm)\n",
    "        x_cm = x_cm.permute(0, 3, 1, 2)\n",
    "        \n",
    "        return x_cm + shortcut\n",
    "\n",
    "class LiteFusionAttention(nn.Module):\n",
    "    def __init__(self, in_channels, filters):\n",
    "        super(LiteFusionAttention, self).__init__()\n",
    "        self.proj1 = nn.Conv2d(in_channels, filters, kernel_size=1)\n",
    "        self.norm = nn.LayerNorm(filters)\n",
    "        self.conv = nn.Conv2d(filters, filters, kernel_size=3, padding=1)\n",
    "        self.fmca = FocalModulationContextAggregation(filters, filters)\n",
    "        self.proj2 = nn.Conv2d(2 * filters, filters, kernel_size=1)\n",
    "        self.vm = VisionMambaInspired(filters)\n",
    "        \n",
    "        self.res_proj = nn.Conv2d(in_channels, filters, kernel_size=1) if in_channels != filters else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_tensor = self.proj1(x)\n",
    "        \n",
    "        x_perm = input_tensor.permute(0, 2, 3, 1)\n",
    "        x_norm = self.norm(x_perm).permute(0, 3, 1, 2)\n",
    "        \n",
    "        x_conv = self.conv(x_norm)\n",
    "        x_fmca = self.fmca(x_conv)\n",
    "        x_proj = self.proj2(x_fmca)\n",
    "        \n",
    "        res = self.res_proj(x) if isinstance(self.res_proj, nn.Conv2d) else input_tensor\n",
    "        out = x_proj + res\n",
    "        \n",
    "        out = self.vm(out)\n",
    "        return out\n",
    "\n",
    "class RA_AttentionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, n_classes, k):\n",
    "        super(RA_AttentionBlock, self).__init__()\n",
    "        self.k = k\n",
    "        self.n_classes = n_classes\n",
    "        self.conv = nn.Conv2d(in_channels, k * n_classes, kernel_size=3, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(k * n_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.gmp = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        f = self.relu(self.bn(self.conv(x)))\n",
    "        \n",
    "        x1 = self.gmp(f)\n",
    "        x2 = self.gap(f)\n",
    "        x_mul = x1 * x2\n",
    "        \n",
    "        x_reshape = x_mul.view(b, self.n_classes, self.k)\n",
    "        s = torch.mean(x_reshape, dim=-1, keepdim=False)\n",
    "        \n",
    "        f_perm = f.permute(0, 2, 3, 1)\n",
    "        f_reshape = f_perm.view(b, h, w, self.n_classes, self.k)\n",
    "        f_mean = torch.mean(f_reshape, dim=-1, keepdim=False)\n",
    "        \n",
    "        s_expanded = s.view(b, 1, 1, self.n_classes)\n",
    "        x_weighted = f_mean * s_expanded\n",
    "        \n",
    "        m = torch.mean(x_weighted, dim=-1, keepdim=True)\n",
    "        m = m.permute(0, 3, 1, 2)\n",
    "        \n",
    "        semantic = x * m\n",
    "        return semantic\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout_rate=0.5):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1x1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0, bias=False)\n",
    "        self.conv3x3 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.conv3x3_dilated = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=2, dilation=2, bias=False)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        c1 = self.conv1x1(x)\n",
    "        c3 = self.conv3x3(x)\n",
    "        c3d = self.conv3x3_dilated(x)\n",
    "        out = c1 + c3 + c3d\n",
    "        out = self.leaky_relu(out)\n",
    "        return out\n",
    "\n",
    "class build_unet(nn.Module):\n",
    "    def __init__(self, input_channels=4, num_classes=1, feature_scale=2, dropout=0.5):\n",
    "        super(build_unet, self).__init__()\n",
    "        filters = [int(x / feature_scale) for x in [16, 32, 64]]\n",
    "        \n",
    "        self.conv1 = ConvBlock(input_channels, filters[0], dropout)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.bn1 = nn.BatchNorm2d(filters[0])\n",
    "        \n",
    "        self.conv2 = ConvBlock(filters[0], filters[1], dropout)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.bn2 = nn.BatchNorm2d(filters[1])\n",
    "        \n",
    "        self.conv3 = ConvBlock(filters[1], filters[2], dropout)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.bn3 = nn.BatchNorm2d(filters[2])\n",
    "        \n",
    "        self.lfa = LiteFusionAttention(filters[2], filters=32)\n",
    "        \n",
    "        lfa_out_channels = 32 \n",
    "        self.att1 = RA_AttentionBlock(lfa_out_channels, 1, 16)\n",
    "        \n",
    "        self.up1 = nn.ConvTranspose2d(lfa_out_channels * 2, filters[2], kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        \n",
    "        self.att2 = RA_AttentionBlock(filters[1], 1, 16)\n",
    "        self.dec_conv1 = nn.Conv2d(filters[1] + filters[2], filters[2], kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.up2 = nn.ConvTranspose2d(filters[2], filters[2], kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        \n",
    "        self.att3 = RA_AttentionBlock(filters[0], 1, 16)\n",
    "        self.dec_conv2 = nn.Conv2d(filters[0] + filters[2], filters[2], kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.up3 = nn.ConvTranspose2d(filters[2], filters[0], kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.dec_conv3 = nn.Conv2d(filters[0], filters[0], kernel_size=3, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.final = nn.Conv2d(filters[0], num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        c1 = self.conv1(x)\n",
    "        p1 = self.bn1(self.pool1(c1))\n",
    "        \n",
    "        c2 = self.conv2(p1)\n",
    "        p2 = self.bn2(self.pool2(c2))\n",
    "        \n",
    "        c3 = self.conv3(p2)\n",
    "        p3 = self.bn3(self.pool3(c3))\n",
    "        \n",
    "        lfa = self.lfa(p3)\n",
    "        \n",
    "        att1 = self.att1(lfa)\n",
    "        fused = torch.cat([att1, lfa], dim=1)\n",
    "        \n",
    "        d1 = self.up1(fused)\n",
    "        if d1.size() != c2.size():\n",
    "             d1 = F.interpolate(d1, size=c2.shape[2:], mode='bilinear', align_corners=True)\n",
    "             \n",
    "        att2 = self.att2(c2)\n",
    "        d1 = torch.cat([att2, d1], dim=1)\n",
    "        d1 = self.relu1(self.dec_conv1(d1))\n",
    "        \n",
    "        d2 = self.up2(d1)\n",
    "        if d2.size() != c1.size():\n",
    "             d2 = F.interpolate(d2, size=c1.shape[2:], mode='bilinear', align_corners=True)\n",
    "\n",
    "        att3 = self.att3(c1)\n",
    "        d2 = torch.cat([att3, d2], dim=1)\n",
    "        d2 = self.relu2(self.dec_conv2(d2))\n",
    "        \n",
    "        d3 = self.up3(d2)\n",
    "        if d3.size() != x.size():\n",
    "             d3 = F.interpolate(d3, size=x.shape[2:], mode='bilinear', align_corners=True)\n",
    "             \n",
    "        d3 = self.relu3(self.dec_conv3(d3))\n",
    "        \n",
    "        out = self.final(d3)\n",
    "        return out\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    x = torch.randn((2, 4, 560, 560))\n",
    "    f = build_unet()\n",
    "    y = f(x)\n",
    "    print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T10:17:40.437315Z",
     "iopub.status.busy": "2025-12-26T10:17:40.437071Z",
     "iopub.status.idle": "2025-12-26T10:17:40.443522Z",
     "shell.execute_reply": "2025-12-26T10:17:40.442839Z",
     "shell.execute_reply.started": "2025-12-26T10:17:40.437294Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DriveDataset(Dataset):\n",
    "    def __init__(self, images_path, masks_path, od_masks_path):\n",
    "        self.images_path = images_path\n",
    "        self.masks_path = masks_path\n",
    "        self.od_masks_path = od_masks_path\n",
    "        self.n_samples = len(images_path)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Reading image \"\"\"\n",
    "        image = cv2.imread(self.images_path[index], cv2.IMREAD_COLOR)\n",
    "        image = image / 255.0 \n",
    "        # (H, W, 3)\n",
    "        \n",
    "        \"\"\" Reading OD mask \"\"\"\n",
    "        od_mask = cv2.imread(self.od_masks_path[index], cv2.IMREAD_GRAYSCALE)\n",
    "        od_mask = od_mask / 255.0\n",
    "        od_mask = np.expand_dims(od_mask, axis=-1) # (H, W, 1)\n",
    "        \n",
    "        \"\"\" Concatenate: 融合 Image 和 OD Mask \"\"\"\n",
    "        # 结果形状: (H, W, 4) ->  transpose -> (4, H, W)\n",
    "        input_tensor = np.concatenate([image, od_mask], axis=-1)\n",
    "        \n",
    "        input_tensor = np.transpose(input_tensor, (2, 0, 1))\n",
    "        input_tensor = input_tensor.astype(np.float32)\n",
    "        input_tensor = torch.from_numpy(input_tensor)\n",
    "\n",
    "        \"\"\" Reading label mask \"\"\"\n",
    "        mask = cv2.imread(self.masks_path[index], cv2.IMREAD_GRAYSCALE)\n",
    "        mask = mask / 255.0\n",
    "        mask = np.expand_dims(mask, axis=0)\n",
    "        mask = mask.astype(np.float32)\n",
    "        mask = torch.from_numpy(mask)\n",
    "\n",
    "        return input_tensor, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T10:17:40.445318Z",
     "iopub.status.busy": "2025-12-26T10:17:40.445100Z",
     "iopub.status.idle": "2025-12-26T10:17:40.467440Z",
     "shell.execute_reply": "2025-12-26T10:17:40.466743Z",
     "shell.execute_reply.started": "2025-12-26T10:17:40.445297Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Loss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "\n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = torch.sigmoid(inputs)\n",
    "\n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n",
    "\n",
    "        return 1 - dice\n",
    "\n",
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "\n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = torch.sigmoid(inputs)\n",
    "\n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n",
    "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
    "        Dice_BCE = BCE + dice_loss\n",
    "\n",
    "        return Dice_BCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T10:17:40.468545Z",
     "iopub.status.busy": "2025-12-26T10:17:40.468339Z",
     "iopub.status.idle": "2025-12-26T10:17:40.482989Z",
     "shell.execute_reply": "2025-12-26T10:17:40.482370Z",
     "shell.execute_reply.started": "2025-12-26T10:17:40.468525Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Utils\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "\"\"\" Seeding the randomness. \"\"\"\n",
    "def seeding(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\"\"\" Create a directory. \"\"\"\n",
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "\"\"\" Calculate the time taken \"\"\"\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T10:17:40.483887Z",
     "iopub.status.busy": "2025-12-26T10:17:40.483664Z",
     "iopub.status.idle": "2025-12-26T10:56:04.787586Z",
     "shell.execute_reply": "2025-12-26T10:56:04.786752Z",
     "shell.execute_reply.started": "2025-12-26T10:17:40.483865Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size:\n",
      "Train: 160 - Valid: 20\n",
      "\n",
      "Valid loss improved from inf to 1.5296. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 01 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 1.532\n",
      "\t Val. Loss: 1.530\n",
      "\n",
      "Valid loss improved from 1.5296 to 1.5244. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 02 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 1.524\n",
      "\t Val. Loss: 1.524\n",
      "\n",
      "Valid loss improved from 1.5244 to 1.5195. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 03 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 1.512\n",
      "\t Val. Loss: 1.520\n",
      "\n",
      "Valid loss improved from 1.5195 to 1.5140. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 04 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 1.473\n",
      "\t Val. Loss: 1.514\n",
      "\n",
      "Valid loss improved from 1.5140 to 1.5043. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 05 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 1.327\n",
      "\t Val. Loss: 1.504\n",
      "\n",
      "Valid loss improved from 1.5043 to 1.4719. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 06 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 1.254\n",
      "\t Val. Loss: 1.472\n",
      "\n",
      "Valid loss improved from 1.4719 to 1.3632. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 07 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 1.213\n",
      "\t Val. Loss: 1.363\n",
      "\n",
      "Valid loss improved from 1.3632 to 1.2336. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 08 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 1.159\n",
      "\t Val. Loss: 1.234\n",
      "\n",
      "Valid loss improved from 1.2336 to 1.1747. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 09 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 1.148\n",
      "\t Val. Loss: 1.175\n",
      "\n",
      "Epoch: 10 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 1.131\n",
      "\t Val. Loss: 1.180\n",
      "\n",
      "Valid loss improved from 1.1747 to 1.1466. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 11 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 1.128\n",
      "\t Val. Loss: 1.147\n",
      "\n",
      "Valid loss improved from 1.1466 to 1.1290. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 12 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 1.116\n",
      "\t Val. Loss: 1.129\n",
      "\n",
      "Valid loss improved from 1.1290 to 1.1217. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 13 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 1.116\n",
      "\t Val. Loss: 1.122\n",
      "\n",
      "Epoch: 14 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 1.104\n",
      "\t Val. Loss: 1.136\n",
      "\n",
      "Valid loss improved from 1.1217 to 1.1112. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 15 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 1.097\n",
      "\t Val. Loss: 1.111\n",
      "\n",
      "Epoch: 16 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 1.084\n",
      "\t Val. Loss: 1.137\n",
      "\n",
      "Valid loss improved from 1.1112 to 1.0911. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 17 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 1.069\n",
      "\t Val. Loss: 1.091\n",
      "\n",
      "Epoch: 18 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 1.053\n",
      "\t Val. Loss: 1.139\n",
      "\n",
      "Epoch: 19 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 1.022\n",
      "\t Val. Loss: 1.101\n",
      "\n",
      "Valid loss improved from 1.0911 to 1.0766. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 20 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.987\n",
      "\t Val. Loss: 1.077\n",
      "\n",
      "Valid loss improved from 1.0766 to 0.9961. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 21 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.964\n",
      "\t Val. Loss: 0.996\n",
      "\n",
      "Valid loss improved from 0.9961 to 0.9808. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 22 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.924\n",
      "\t Val. Loss: 0.981\n",
      "\n",
      "Epoch: 23 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.908\n",
      "\t Val. Loss: 0.987\n",
      "\n",
      "Valid loss improved from 0.9808 to 0.8409. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 24 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.834\n",
      "\t Val. Loss: 0.841\n",
      "\n",
      "Valid loss improved from 0.8409 to 0.8329. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 25 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.790\n",
      "\t Val. Loss: 0.833\n",
      "\n",
      "Valid loss improved from 0.8329 to 0.7950. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 26 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.767\n",
      "\t Val. Loss: 0.795\n",
      "\n",
      "Valid loss improved from 0.7950 to 0.7372. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 27 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.745\n",
      "\t Val. Loss: 0.737\n",
      "\n",
      "Valid loss improved from 0.7372 to 0.7157. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 28 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.725\n",
      "\t Val. Loss: 0.716\n",
      "\n",
      "Valid loss improved from 0.7157 to 0.6746. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 29 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.706\n",
      "\t Val. Loss: 0.675\n",
      "\n",
      "Valid loss improved from 0.6746 to 0.6741. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 30 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.694\n",
      "\t Val. Loss: 0.674\n",
      "\n",
      "Valid loss improved from 0.6741 to 0.6637. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 31 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.683\n",
      "\t Val. Loss: 0.664\n",
      "\n",
      "Valid loss improved from 0.6637 to 0.6468. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 32 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.668\n",
      "\t Val. Loss: 0.647\n",
      "\n",
      "Epoch: 33 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.647\n",
      "\t Val. Loss: 0.654\n",
      "\n",
      "Valid loss improved from 0.6468 to 0.6097. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 34 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.638\n",
      "\t Val. Loss: 0.610\n",
      "\n",
      "Valid loss improved from 0.6097 to 0.5944. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 35 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.623\n",
      "\t Val. Loss: 0.594\n",
      "\n",
      "Valid loss improved from 0.5944 to 0.5858. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 36 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.609\n",
      "\t Val. Loss: 0.586\n",
      "\n",
      "Epoch: 37 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.600\n",
      "\t Val. Loss: 0.591\n",
      "\n",
      "Epoch: 38 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.583\n",
      "\t Val. Loss: 0.590\n",
      "\n",
      "Valid loss improved from 0.5858 to 0.5793. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 39 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.569\n",
      "\t Val. Loss: 0.579\n",
      "\n",
      "Epoch: 40 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.583\n",
      "\t Val. Loss: 0.616\n",
      "\n",
      "Epoch: 41 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.566\n",
      "\t Val. Loss: 0.602\n",
      "\n",
      "Valid loss improved from 0.5793 to 0.5746. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 42 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.564\n",
      "\t Val. Loss: 0.575\n",
      "\n",
      "Valid loss improved from 0.5746 to 0.5413. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 43 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.547\n",
      "\t Val. Loss: 0.541\n",
      "\n",
      "Valid loss improved from 0.5413 to 0.5358. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 44 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.538\n",
      "\t Val. Loss: 0.536\n",
      "\n",
      "Epoch: 45 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.530\n",
      "\t Val. Loss: 0.537\n",
      "\n",
      "Epoch: 46 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.520\n",
      "\t Val. Loss: 0.545\n",
      "\n",
      "Epoch: 47 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.515\n",
      "\t Val. Loss: 0.543\n",
      "\n",
      "Epoch: 48 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.509\n",
      "\t Val. Loss: 0.578\n",
      "\n",
      "Epoch: 49 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.499\n",
      "\t Val. Loss: 0.570\n",
      "\n",
      "Epoch: 50 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.498\n",
      "\t Val. Loss: 0.550\n",
      "\n",
      "Valid loss improved from 0.5358 to 0.4821. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 51 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.496\n",
      "\t Val. Loss: 0.482\n",
      "\n",
      "Valid loss improved from 0.4821 to 0.4574. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 52 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.504\n",
      "\t Val. Loss: 0.457\n",
      "\n",
      "Epoch: 53 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.492\n",
      "\t Val. Loss: 0.484\n",
      "\n",
      "Epoch: 54 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.496\n",
      "\t Val. Loss: 0.473\n",
      "\n",
      "Valid loss improved from 0.4574 to 0.4548. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 55 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.490\n",
      "\t Val. Loss: 0.455\n",
      "\n",
      "Epoch: 56 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.485\n",
      "\t Val. Loss: 0.469\n",
      "\n",
      "Valid loss improved from 0.4548 to 0.4505. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 57 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.482\n",
      "\t Val. Loss: 0.450\n",
      "\n",
      "Epoch: 58 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.483\n",
      "\t Val. Loss: 0.468\n",
      "\n",
      "Valid loss improved from 0.4505 to 0.4489. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 59 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.480\n",
      "\t Val. Loss: 0.449\n",
      "\n",
      "Epoch: 60 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.480\n",
      "\t Val. Loss: 0.465\n",
      "\n",
      "Epoch: 61 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.479\n",
      "\t Val. Loss: 0.605\n",
      "\n",
      "Valid loss improved from 0.4489 to 0.4469. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 62 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.482\n",
      "\t Val. Loss: 0.447\n",
      "\n",
      "Valid loss improved from 0.4469 to 0.4377. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 63 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.471\n",
      "\t Val. Loss: 0.438\n",
      "\n",
      "Epoch: 64 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.464\n",
      "\t Val. Loss: 0.458\n",
      "\n",
      "Valid loss improved from 0.4377 to 0.4293. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 65 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.469\n",
      "\t Val. Loss: 0.429\n",
      "\n",
      "Epoch: 66 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.463\n",
      "\t Val. Loss: 0.471\n",
      "\n",
      "Epoch: 67 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.460\n",
      "\t Val. Loss: 0.460\n",
      "\n",
      "Epoch: 68 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.460\n",
      "\t Val. Loss: 0.436\n",
      "\n",
      "Epoch: 69 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.459\n",
      "\t Val. Loss: 0.440\n",
      "\n",
      "Epoch: 70 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.458\n",
      "\t Val. Loss: 0.432\n",
      "\n",
      "Epoch: 71 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.458\n",
      "\t Val. Loss: 0.448\n",
      "\n",
      "Valid loss improved from 0.4293 to 0.4258. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 72 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.455\n",
      "\t Val. Loss: 0.426\n",
      "\n",
      "Epoch: 73 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.453\n",
      "\t Val. Loss: 0.434\n",
      "\n",
      "Valid loss improved from 0.4258 to 0.4193. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 74 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.450\n",
      "\t Val. Loss: 0.419\n",
      "\n",
      "Epoch: 75 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.449\n",
      "\t Val. Loss: 0.426\n",
      "\n",
      "Epoch: 76 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.454\n",
      "\t Val. Loss: 0.420\n",
      "\n",
      "Epoch: 77 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.449\n",
      "\t Val. Loss: 0.425\n",
      "\n",
      "Epoch: 78 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.449\n",
      "\t Val. Loss: 0.441\n",
      "\n",
      "Epoch: 79 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.449\n",
      "\t Val. Loss: 0.422\n",
      "\n",
      "Epoch: 80 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.457\n",
      "\t Val. Loss: 0.425\n",
      "\n",
      "Epoch: 81 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.452\n",
      "\t Val. Loss: 0.431\n",
      "\n",
      "Valid loss improved from 0.4193 to 0.4169. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 82 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.448\n",
      "\t Val. Loss: 0.417\n",
      "\n",
      "Epoch: 83 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.442\n",
      "\t Val. Loss: 0.425\n",
      "\n",
      "Valid loss improved from 0.4169 to 0.4141. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 84 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.441\n",
      "\t Val. Loss: 0.414\n",
      "\n",
      "Epoch: 85 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.441\n",
      "\t Val. Loss: 0.419\n",
      "\n",
      "Epoch: 86 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.442\n",
      "\t Val. Loss: 0.418\n",
      "\n",
      "Epoch: 87 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.438\n",
      "\t Val. Loss: 0.417\n",
      "\n",
      "Valid loss improved from 0.4141 to 0.4136. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 88 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.439\n",
      "\t Val. Loss: 0.414\n",
      "\n",
      "Valid loss improved from 0.4136 to 0.4106. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 89 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.434\n",
      "\t Val. Loss: 0.411\n",
      "\n",
      "Valid loss improved from 0.4106 to 0.4089. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 90 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.439\n",
      "\t Val. Loss: 0.409\n",
      "\n",
      "Epoch: 91 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.435\n",
      "\t Val. Loss: 0.410\n",
      "\n",
      "Valid loss improved from 0.4089 to 0.4075. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 92 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.436\n",
      "\t Val. Loss: 0.407\n",
      "\n",
      "Epoch: 93 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.433\n",
      "\t Val. Loss: 0.408\n",
      "\n",
      "Epoch: 94 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.432\n",
      "\t Val. Loss: 0.410\n",
      "\n",
      "Valid loss improved from 0.4075 to 0.4047. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 95 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.438\n",
      "\t Val. Loss: 0.405\n",
      "\n",
      "Epoch: 96 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.432\n",
      "\t Val. Loss: 0.412\n",
      "\n",
      "Epoch: 97 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.432\n",
      "\t Val. Loss: 0.417\n",
      "\n",
      "Epoch: 98 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.430\n",
      "\t Val. Loss: 0.409\n",
      "\n",
      "Valid loss improved from 0.4047 to 0.4030. Saving checkpoint: working/files/drive_checkpoint.pth\n",
      "Epoch: 99 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.429\n",
      "\t Val. Loss: 0.403\n",
      "\n",
      "Epoch: 100 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.428\n",
      "\t Val. Loss: 0.410\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train\n",
    "import os\n",
    "import time\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "# from data import DriveDataset\n",
    "# from model import build_unet\n",
    "# from loss import DiceLoss, DiceBCELoss\n",
    "# from utils import seeding, create_dir, epoch_time\n",
    "\n",
    "'''训练深度学习模型'''\n",
    "def train(model, loader, optimizer, loss_fn, device, show_images=False):\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    model.train()\n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        x = x.to(device, dtype=torch.float32)\n",
    "        y = y.to(device, dtype=torch.float32)\n",
    "\n",
    "        # if i == 1 and show_images:\n",
    "        #    # 显示第一批图像和掩码\n",
    "        #     img = x[0].cpu().numpy()  # 假设图像是CHW格式\n",
    "        #     img = np.transpose(img, (1, 2, 0))  # 转换为HWC格式\n",
    "        #     img = img[..., ::-1]  # 将BGR转换为RGB\n",
    "        #     mask = y[0].cpu().numpy()  # 假设掩码是CHW格式\n",
    "        #     mask = np.transpose(mask, (1, 2, 0))  # 转换为HWC格式\n",
    "\n",
    "        #     plt.figure(figsize=(12, 6))\n",
    "\n",
    "        #     plt.subplot(1, 2, 1)\n",
    "        #     plt.imshow(img)\n",
    "        #     plt.title(\"Sample Image\")\n",
    "\n",
    "        #     plt.subplot(1, 2, 2)\n",
    "        #     plt.imshow(mask, cmap='gray')\n",
    "        #     plt.title(\"Corresponding Mask\")\n",
    "\n",
    "        #     plt.show()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    #计算整个epoch的平均损失\n",
    "    epoch_loss = epoch_loss/len(loader)\n",
    "    return epoch_loss\n",
    "\n",
    "def evaluate(model, loader, loss_fn, device):\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device, dtype=torch.float32)\n",
    "            y = y.to(device, dtype=torch.float32)\n",
    "\n",
    "            y_pred = model(x)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        epoch_loss = epoch_loss/len(loader)\n",
    "    return epoch_loss\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\" Seeding \"\"\"\n",
    "    seeding(42)\n",
    "\n",
    "    \"\"\" Directories \"\"\"\n",
    "    create_dir(\"files\")\n",
    "\n",
    "    \"\"\" Load dataset \"\"\"\n",
    "    train_x = sorted(glob(\"working/new_data/train/image/*\"))\n",
    "    train_y = sorted(glob(\"working/new_data/train/mask/*\"))\n",
    "    train_od = sorted(glob(\"working/new_data/train/od_mask/*\")) # 加载增强后的 OD Mask\n",
    "\n",
    "    valid_x = sorted(glob(\"working/new_data/test/image/*\"))\n",
    "    valid_y = sorted(glob(\"working/new_data/test/mask/*\"))\n",
    "    valid_od = sorted(glob(\"working/new_data/test/od_mask/*\"))\n",
    "\n",
    "    data_str = f\"Dataset Size:\\nTrain: {len(train_x)} - Valid: {len(valid_x)}\\n\"\n",
    "    print(data_str)\n",
    "\n",
    "    \"\"\" Hyperparameters \"\"\"\n",
    "    H = 560\n",
    "    W = 560\n",
    "    size = (H, W)\n",
    "    batch_size = 64\n",
    "    num_epochs = 100   \n",
    "    lr = 1e-3\n",
    "    checkpoint_path = \"working/files/drive_checkpoint.pth\"\n",
    "\n",
    "    \"\"\" Dataset and loader \"\"\"\n",
    "    train_dataset = DriveDataset(train_x, train_y, train_od)\n",
    "    valid_dataset = DriveDataset(valid_x, valid_y, valid_od)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        dataset=valid_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   ## GTX 1060 6GB\n",
    "    model = build_unet()\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
    "    loss_fn = DiceBCELoss()\n",
    "\n",
    "    \"\"\" Training the model \"\"\"\n",
    "    best_valid_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        #该轮训练的平均损失值 train_loss\n",
    "        train_loss = train(model, train_loader, optimizer, loss_fn, device, show_images=True)\n",
    "        #返回验证损失 valid_loss\n",
    "        valid_loss = evaluate(model, valid_loader, loss_fn, device)\n",
    "\n",
    "        \"\"\" Saving the model \"\"\"\n",
    "        if valid_loss < best_valid_loss:\n",
    "            data_str = f\"Valid loss improved from {best_valid_loss:2.4f} to {valid_loss:2.4f}. Saving checkpoint: {checkpoint_path}\"\n",
    "            print(data_str)\n",
    "\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        data_str = f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\\n'\n",
    "        data_str += f'\\tTrain Loss: {train_loss:.3f}\\n'\n",
    "        data_str += f'\\t Val. Loss: {valid_loss:.3f}\\n'\n",
    "        print(data_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T10:56:04.789919Z",
     "iopub.status.busy": "2025-12-26T10:56:04.789291Z",
     "iopub.status.idle": "2025-12-26T10:56:09.112965Z",
     "shell.execute_reply": "2025-12-26T10:56:09.112196Z",
     "shell.execute_reply.started": "2025-12-26T10:56:04.789886Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_599743/3027964216.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 16.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall---Jaccard: 0.6240 - F1: 0.7681 - Sensitivity: 0.7490 - Precision: 0.7945 - Acc: 0.9608 - Specificity:0.9813\n",
      "FPS:  378.17355591721173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "import os, time\n",
    "from operator import add\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# 假设 build_unet, create_dir, seeding 等函数已经在上下文中定义\n",
    "# 如果是在 notebook 中，确保上面的 cell 已经运行\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\" 计算指标 (保持不变) \"\"\"\n",
    "    y_true = y_true.cpu().numpy()\n",
    "    y_true = y_true > 0.5\n",
    "    y_true = y_true.astype(np.uint8)\n",
    "    y_true = y_true.reshape(-1)\n",
    "\n",
    "    y_pred = y_pred.cpu().numpy()\n",
    "    y_pred = y_pred > 0.5\n",
    "    y_pred = y_pred.astype(np.uint8)\n",
    "    y_pred = y_pred.reshape(-1)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "\n",
    "    score_f1 = 2 * tp / (2 * tp + fp + fn + 1e-6)\n",
    "    score_jaccard = tp / (tp + fp + fn + 1e-6)\n",
    "    score_recall = tp / (tp + fn + 1e-6)\n",
    "    score_specificity = tn / (tn + fp + 1e-6)\n",
    "    score_precision = tp / (tp + fp + 1e-6)\n",
    "    score_acc = (tp + tn) / (tp + tn + fp + fn + 1e-6)\n",
    "\n",
    "    return [score_jaccard, score_f1, score_recall, score_precision, score_acc, score_specificity]\n",
    "\n",
    "def mask_parse(mask):\n",
    "    mask = np.expand_dims(mask, axis=-1)\n",
    "    mask = np.concatenate([mask, mask, mask], axis=-1)\n",
    "    return mask\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\" Seeding \"\"\"\n",
    "    seeding(42)\n",
    "\n",
    "    \"\"\" Folders \"\"\"\n",
    "    create_dir(\"working/results\")\n",
    "\n",
    "    \"\"\" Load dataset \"\"\"\n",
    "    # 1. 加载三个路径列表：图像、金标准Mask、视盘Mask\n",
    "    test_x = sorted(glob(\"working/new_data/test/image/*\"))\n",
    "    test_y = sorted(glob(\"working/new_data/test/mask/*\"))\n",
    "    test_od = sorted(glob(\"working/new_data/test/od_mask/*\")) # 【新增】加载视盘Mask路径\n",
    "\n",
    "    # 检查文件数量是否匹配\n",
    "    if len(test_x) != len(test_od):\n",
    "        print(f\"警告: 图像数量 ({len(test_x)}) 与视盘Mask数量 ({len(test_od)}) 不匹配！\")\n",
    "\n",
    "    \"\"\" Hyperparameters \"\"\"\n",
    "    H = 560\n",
    "    W = 560\n",
    "    size = (W, H)\n",
    "    checkpoint_path = \"working/files/drive_checkpoint_od.pth\"\n",
    "\n",
    "    \"\"\" Load the checkpoint \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # 初始化模型时，确保 input_channels=4\n",
    "    model = build_unet(input_channels=4) \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # 如果训练时用了 DataParallel，这里加载权重可能需要处理 'module.' 前缀\n",
    "    # 这里假设直接加载\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    metrics_score = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "    time_taken = []\n",
    "    \n",
    "    # 遍历 Image, Label, OD_Mask\n",
    "    for i, (x_path, y_path, od_path) in tqdm(enumerate(zip(test_x, test_y, test_od)), total=len(test_x)):\n",
    "        \"\"\" Extract the name \"\"\"\n",
    "        name = x_path.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "        \"\"\" 1. 处理原图 (RGB) \"\"\"\n",
    "        image = cv2.imread(x_path, cv2.IMREAD_COLOR) \n",
    "        # image 已经是 560x560 (数据增强阶段处理过)\n",
    "        x = np.transpose(image, (2, 0, 1))      ## (3, 560, 560)\n",
    "        x = x / 255.0\n",
    "        \n",
    "        \"\"\" 2. 处理视盘 Mask (OD) \"\"\"\n",
    "        od_mask = cv2.imread(od_path, cv2.IMREAD_GRAYSCALE) # 读取单通道\n",
    "        # od_mask 已经是 560x560\n",
    "        od_mask = od_mask / 255.0\n",
    "        od_mask = np.expand_dims(od_mask, axis=0) ## (1, 560, 560)\n",
    "\n",
    "        \"\"\" 3. 合并通道 (Concatenate) \"\"\"\n",
    "        # 将 RGB (3, H, W) 和 OD (1, H, W) 在通道维度合并 -> (4, H, W)\n",
    "        x_concat = np.concatenate([x, od_mask], axis=0)\n",
    "        \n",
    "        # 转为 Tensor 并增加 Batch 维度 -> (1, 4, 560, 560)\n",
    "        x_tensor = np.expand_dims(x_concat, axis=0) \n",
    "        x_tensor = x_tensor.astype(np.float32)\n",
    "        x_tensor = torch.from_numpy(x_tensor).to(device)\n",
    "\n",
    "        \"\"\" Reading label mask \"\"\"\n",
    "        mask = cv2.imread(y_path, cv2.IMREAD_GRAYSCALE)\n",
    "        y = np.expand_dims(mask, axis=0)            \n",
    "        y = y / 255.0\n",
    "        y = np.expand_dims(y, axis=0)               \n",
    "        y = y.astype(np.float32)\n",
    "        y = torch.from_numpy(y).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \"\"\" Prediction \"\"\"\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # 模型输入现在是 4 通道，不会报错了\n",
    "            pred_y = model(x_tensor) \n",
    "            pred_y = torch.sigmoid(pred_y)\n",
    "            \n",
    "            total_time = time.time() - start_time\n",
    "            time_taken.append(total_time)\n",
    "\n",
    "            score = calculate_metrics(y, pred_y)\n",
    "            # print(f\":-- jaccard:{score[0]:1.4f} ...\") \n",
    "            \n",
    "            metrics_score = list(map(add, metrics_score, score))\n",
    "            \n",
    "            # 后处理用于保存图片\n",
    "            pred_y = pred_y[0].cpu().numpy()        \n",
    "            pred_y = np.squeeze(pred_y, axis=0)     \n",
    "            pred_y = pred_y > 0.5\n",
    "            pred_y = np.array(pred_y, dtype=np.uint8)\n",
    "\n",
    "        \"\"\" Saving masks \"\"\"\n",
    "        ori_mask = mask_parse(mask)\n",
    "        pred_y_viz = mask_parse(pred_y)\n",
    "        line = np.ones((size[1], 10, 3)) * 128\n",
    "\n",
    "        # 拼接图片用于展示结果\n",
    "        cat_images = np.concatenate(\n",
    "            [image, line, ori_mask, line, pred_y_viz * 255], axis=1\n",
    "        )\n",
    "        cv2.imwrite(f\"working/results/{name}.png\", cat_images)\n",
    "\n",
    "    jaccard = metrics_score[0]/len(test_x)\n",
    "    f1 = metrics_score[1]/len(test_x)\n",
    "    recall = metrics_score[2]/len(test_x)\n",
    "    precision = metrics_score[3]/len(test_x)\n",
    "    acc = metrics_score[4]/len(test_x)\n",
    "    spec = metrics_score[5]/len(test_x)\n",
    "    print(f\"\\nOverall---Jaccard: {jaccard:1.4f} - F1: {f1:1.4f} - Sensitivity: {recall:1.4f} - Precision: {precision:1.4f} - Acc: {acc:1.4f} - Specificity:{spec:1.4f}\")\n",
    "\n",
    "    fps = 1/np.mean(time_taken)\n",
    "    print(\"FPS: \", fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 946814,
     "sourceId": 1604025,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "pampc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
