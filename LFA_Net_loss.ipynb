{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T10:17:27.187106Z",
     "iopub.status.busy": "2025-12-26T10:17:27.186304Z",
     "iopub.status.idle": "2025-12-26T10:17:38.866947Z",
     "shell.execute_reply": "2025-12-26T10:17:38.866281Z",
     "shell.execute_reply.started": "2025-12-26T10:17:27.187068Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangziteng/miniconda3/envs/pampc/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 20 - 20\n",
      "Test: 20 - 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  9.45it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 55.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# Data Augmentation\n",
    "import sys\n",
    "import os\n",
    "\n",
    "if os.path.exists(\"config.py\"):\n",
    "    print(\"Warning: Found 'config.py' in the current directory. Please rename it to avoid conflicts with torch.\")\n",
    "if os.path.exists(\"torch.py\"):\n",
    "    print(\"Warning: Found 'torch.py' in the current directory. Please rename it to avoid conflicts with torch.\")\n",
    "\n",
    "# 优先导入 torch\n",
    "import torch\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import imageio\n",
    "from albumentations import HorizontalFlip, VerticalFlip, Rotate, RandomBrightnessContrast, RandomCrop, RandomRotate90, RandomGridShuffle\n",
    "\n",
    "\"\"\" Create a directory \"\"\"\n",
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "'''加载数据：原图+标签'''\n",
    "def load_data(path):\n",
    "    train_x = sorted(glob(os.path.join(path, \"training\", \"images\", \"*.tif\")))\n",
    "    train_y = sorted(glob(os.path.join(path, \"training\", \"1st_manual\", \"*.gif\")))\n",
    "\n",
    "    test_x = sorted(glob(os.path.join(path, \"test\", \"images\", \"*.tif\")))\n",
    "    test_y = sorted(glob(os.path.join(path, \"test\", \"1st_manual\", \"*.gif\")))\n",
    "\n",
    "    return (train_x, train_y), (test_x, test_y)\n",
    "\n",
    "'''\n",
    "增强数据\n",
    "对图像及其对应mask数据增强\n",
    "'''\n",
    "def augment_data(images, masks, save_path, augment=True):\n",
    "    size = (560, 560)\n",
    "\n",
    "    for idx, (x, y) in tqdm(enumerate(zip(images, masks)), total=len(images)):\n",
    "        \"\"\" Extracting the name \"\"\"\n",
    "        name = x.split(os.sep)[-1].split(\".\")[0]\n",
    "        \n",
    "        \"\"\" Reading image and mask \"\"\"\n",
    "        x = cv2.imread(x, cv2.IMREAD_COLOR)\n",
    "        y = imageio.mimread(y)[0]\n",
    "\n",
    "        if x is None or y is None:\n",
    "            print(f\"Error reading image or mask for {name}\")\n",
    "            continue\n",
    "\n",
    "        if augment == True:\n",
    "            transformations = [\n",
    "                HorizontalFlip(p=1.0),\n",
    "                VerticalFlip(p=1.0),\n",
    "                Rotate(limit=45, p=1.0),\n",
    "                RandomBrightnessContrast(p=0.6),\n",
    "                RandomCrop(300, 300, p=0.8),\n",
    "                RandomRotate90(p=1.0),\n",
    "                RandomGridShuffle(p=0.7)\n",
    "            ]\n",
    "\n",
    "            augmented_images = [x]\n",
    "            augmented_masks = [y]\n",
    "\n",
    "            for aug in transformations:\n",
    "                augmented = aug(image=x, mask=y)\n",
    "                augmented_images.append(augmented[\"image\"])\n",
    "                augmented_masks.append(augmented[\"mask\"])\n",
    "            \n",
    "            X = augmented_images\n",
    "            Y = augmented_masks\n",
    "\n",
    "        else:\n",
    "            X = [x]\n",
    "            Y = [y]\n",
    "\n",
    "        index = 0\n",
    "        for i, m in zip(X, Y):\n",
    "            #将图像和掩码调整到目标大小\n",
    "            i = cv2.resize(i, size)\n",
    "            m = cv2.resize(m, size)\n",
    "\n",
    "            tmp_image_name = f\"{name}_{index}.png\"\n",
    "            tmp_mask_name = f\"{name}_{index}.png\"\n",
    "\n",
    "            image_path = os.path.join(save_path, \"image\", tmp_image_name)\n",
    "            mask_path = os.path.join(save_path, \"mask\", tmp_mask_name)\n",
    "\n",
    "            cv2.imwrite(image_path, i)\n",
    "            cv2.imwrite(mask_path, m)\n",
    "\n",
    "            index += 1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\" Seeding \"\"\"\n",
    "    np.random.seed(42)\n",
    "\n",
    "    \"\"\" Load the data \"\"\"\n",
    "    # 修改为相对路径\n",
    "    data_path = \"data/DRIVE/\"\n",
    "    \n",
    "    if os.path.exists(data_path):\n",
    "        (train_x, train_y), (test_x, test_y) = load_data(data_path)\n",
    "\n",
    "        print(f\"Train: {len(train_x)} - {len(train_y)}\")\n",
    "        print(f\"Test: {len(test_x)} - {len(test_y)}\")\n",
    "\n",
    "        \"\"\" Create directories to save the augmented data \"\"\"\n",
    "        create_dir(\"working/new_data/train/image/\")\n",
    "        create_dir(\"working/new_data/train/mask/\")\n",
    "        create_dir(\"working/new_data/test/image/\")\n",
    "        create_dir(\"working/new_data/test/mask/\")\n",
    "\n",
    "        \"\"\" Data augmentation \"\"\"\n",
    "        # 取消注释以运行数据增强\n",
    "        augment_data(train_x, train_y, \"working/new_data/train/\", augment=True)\n",
    "        augment_data(test_x, test_y, \"working/new_data/test/\", augment=False)\n",
    "    else:\n",
    "        print(f\"Data path not found: {data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T10:17:38.868551Z",
     "iopub.status.busy": "2025-12-26T10:17:38.868200Z",
     "iopub.status.idle": "2025-12-26T10:17:40.435461Z",
     "shell.execute_reply": "2025-12-26T10:17:40.434747Z",
     "shell.execute_reply.started": "2025-12-26T10:17:38.868526Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 560, 560])\n"
     ]
    }
   ],
   "source": [
    "# Model (LFA-Net)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalModulation(nn.Module):\n",
    "    def __init__(self, in_channels, gamma=2.0, alpha=0.25):\n",
    "        super(FocalModulation, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.gmp = nn.AdaptiveMaxPool2d(1)\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = self.gap(x)\n",
    "        max_val = self.gmp(x)\n",
    "        modulation = (max_val - mean) * self.alpha\n",
    "        modulation = self.conv(modulation)\n",
    "        modulation = self.sigmoid(modulation)\n",
    "        scaled_inputs = x * modulation\n",
    "        outputs = torch.pow(scaled_inputs, self.gamma)\n",
    "        return outputs\n",
    "\n",
    "class FocalModulationContextAggregation(nn.Module):\n",
    "    def __init__(self, in_channels, filters):\n",
    "        super(FocalModulationContextAggregation, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, filters, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(in_channels, filters, kernel_size=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv_ctx = nn.Conv2d(filters, filters, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.focal_mod = FocalModulation(filters)\n",
    "\n",
    "    def forward(self, x):\n",
    "        c1 = self.relu1(self.conv1(x))\n",
    "        c2 = self.relu2(self.conv2(x))\n",
    "        \n",
    "        global_context = self.gap(c2)\n",
    "        global_context = self.sigmoid(self.conv_ctx(global_context))\n",
    "        global_context = c1 * global_context\n",
    "        \n",
    "        fm = self.focal_mod(global_context)\n",
    "        return torch.cat([c1, fm], dim=1)\n",
    "\n",
    "class VisionMambaInspired(nn.Module):\n",
    "    def __init__(self, dim, dropout_rate=0.1):\n",
    "        super(VisionMambaInspired, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.token_mixer = nn.Sequential(\n",
    "            nn.Conv2d(dim, dim, kernel_size=3, padding=1, groups=dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.channel_mixer = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(dim * 4, dim),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        \n",
    "        shortcut = x\n",
    "        x_perm = x.permute(0, 2, 3, 1)\n",
    "        x_norm = self.norm1(x_perm).permute(0, 3, 1, 2)\n",
    "        x_tm = self.token_mixer(x_norm) + shortcut\n",
    "        \n",
    "        shortcut = x_tm\n",
    "        x_perm = x_tm.permute(0, 2, 3, 1)\n",
    "        x_norm = self.norm2(x_perm)\n",
    "        x_cm = self.channel_mixer(x_norm)\n",
    "        x_cm = x_cm.permute(0, 3, 1, 2)\n",
    "        \n",
    "        return x_cm + shortcut\n",
    "\n",
    "class LiteFusionAttention(nn.Module):\n",
    "    def __init__(self, in_channels, filters):\n",
    "        super(LiteFusionAttention, self).__init__()\n",
    "        self.proj1 = nn.Conv2d(in_channels, filters, kernel_size=1)\n",
    "        self.norm = nn.LayerNorm(filters)\n",
    "        self.conv = nn.Conv2d(filters, filters, kernel_size=3, padding=1)\n",
    "        self.fmca = FocalModulationContextAggregation(filters, filters)\n",
    "        self.proj2 = nn.Conv2d(2 * filters, filters, kernel_size=1)\n",
    "        self.vm = VisionMambaInspired(filters)\n",
    "        \n",
    "        self.res_proj = nn.Conv2d(in_channels, filters, kernel_size=1) if in_channels != filters else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_tensor = self.proj1(x)\n",
    "        \n",
    "        x_perm = input_tensor.permute(0, 2, 3, 1)\n",
    "        x_norm = self.norm(x_perm).permute(0, 3, 1, 2)\n",
    "        \n",
    "        x_conv = self.conv(x_norm)\n",
    "        x_fmca = self.fmca(x_conv)\n",
    "        x_proj = self.proj2(x_fmca)\n",
    "        \n",
    "        res = self.res_proj(x) if isinstance(self.res_proj, nn.Conv2d) else input_tensor\n",
    "        out = x_proj + res\n",
    "        \n",
    "        out = self.vm(out)\n",
    "        return out\n",
    "\n",
    "class RA_AttentionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, n_classes, k):\n",
    "        super(RA_AttentionBlock, self).__init__()\n",
    "        self.k = k\n",
    "        self.n_classes = n_classes\n",
    "        self.conv = nn.Conv2d(in_channels, k * n_classes, kernel_size=3, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(k * n_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.gmp = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        f = self.relu(self.bn(self.conv(x)))\n",
    "        \n",
    "        x1 = self.gmp(f)\n",
    "        x2 = self.gap(f)\n",
    "        x_mul = x1 * x2\n",
    "        \n",
    "        x_reshape = x_mul.view(b, self.n_classes, self.k)\n",
    "        s = torch.mean(x_reshape, dim=-1, keepdim=False)\n",
    "        \n",
    "        f_perm = f.permute(0, 2, 3, 1)\n",
    "        f_reshape = f_perm.view(b, h, w, self.n_classes, self.k)\n",
    "        f_mean = torch.mean(f_reshape, dim=-1, keepdim=False)\n",
    "        \n",
    "        s_expanded = s.view(b, 1, 1, self.n_classes)\n",
    "        x_weighted = f_mean * s_expanded\n",
    "        \n",
    "        m = torch.mean(x_weighted, dim=-1, keepdim=True)\n",
    "        m = m.permute(0, 3, 1, 2)\n",
    "        \n",
    "        semantic = x * m\n",
    "        return semantic\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout_rate=0.5):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1x1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0, bias=False)\n",
    "        self.conv3x3 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.conv3x3_dilated = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=2, dilation=2, bias=False)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        c1 = self.conv1x1(x)\n",
    "        c3 = self.conv3x3(x)\n",
    "        c3d = self.conv3x3_dilated(x)\n",
    "        out = c1 + c3 + c3d\n",
    "        out = self.leaky_relu(out)\n",
    "        return out\n",
    "\n",
    "class build_unet(nn.Module):\n",
    "    def __init__(self, input_channels=3, num_classes=1, feature_scale=2, dropout=0.5):\n",
    "        super(build_unet, self).__init__()\n",
    "        filters = [int(x / feature_scale) for x in [16, 32, 64]]\n",
    "        \n",
    "        self.conv1 = ConvBlock(input_channels, filters[0], dropout)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.bn1 = nn.BatchNorm2d(filters[0])\n",
    "        \n",
    "        self.conv2 = ConvBlock(filters[0], filters[1], dropout)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.bn2 = nn.BatchNorm2d(filters[1])\n",
    "        \n",
    "        self.conv3 = ConvBlock(filters[1], filters[2], dropout)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.bn3 = nn.BatchNorm2d(filters[2])\n",
    "        \n",
    "        self.lfa = LiteFusionAttention(filters[2], filters=32)\n",
    "        \n",
    "        lfa_out_channels = 32 \n",
    "        self.att1 = RA_AttentionBlock(lfa_out_channels, 1, 16)\n",
    "        \n",
    "        self.up1 = nn.ConvTranspose2d(lfa_out_channels * 2, filters[2], kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        \n",
    "        self.att2 = RA_AttentionBlock(filters[1], 1, 16)\n",
    "        self.dec_conv1 = nn.Conv2d(filters[1] + filters[2], filters[2], kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.up2 = nn.ConvTranspose2d(filters[2], filters[2], kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        \n",
    "        self.att3 = RA_AttentionBlock(filters[0], 1, 16)\n",
    "        self.dec_conv2 = nn.Conv2d(filters[0] + filters[2], filters[2], kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.up3 = nn.ConvTranspose2d(filters[2], filters[0], kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.dec_conv3 = nn.Conv2d(filters[0], filters[0], kernel_size=3, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.final = nn.Conv2d(filters[0], num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        c1 = self.conv1(x)\n",
    "        p1 = self.bn1(self.pool1(c1))\n",
    "        \n",
    "        c2 = self.conv2(p1)\n",
    "        p2 = self.bn2(self.pool2(c2))\n",
    "        \n",
    "        c3 = self.conv3(p2)\n",
    "        p3 = self.bn3(self.pool3(c3))\n",
    "        \n",
    "        lfa = self.lfa(p3)\n",
    "        \n",
    "        att1 = self.att1(lfa)\n",
    "        fused = torch.cat([att1, lfa], dim=1)\n",
    "        \n",
    "        d1 = self.up1(fused)\n",
    "        if d1.size() != c2.size():\n",
    "             d1 = F.interpolate(d1, size=c2.shape[2:], mode='bilinear', align_corners=True)\n",
    "             \n",
    "        att2 = self.att2(c2)\n",
    "        d1 = torch.cat([att2, d1], dim=1)\n",
    "        d1 = self.relu1(self.dec_conv1(d1))\n",
    "        \n",
    "        d2 = self.up2(d1)\n",
    "        if d2.size() != c1.size():\n",
    "             d2 = F.interpolate(d2, size=c1.shape[2:], mode='bilinear', align_corners=True)\n",
    "\n",
    "        att3 = self.att3(c1)\n",
    "        d2 = torch.cat([att3, d2], dim=1)\n",
    "        d2 = self.relu2(self.dec_conv2(d2))\n",
    "        \n",
    "        d3 = self.up3(d2)\n",
    "        if d3.size() != x.size():\n",
    "             d3 = F.interpolate(d3, size=x.shape[2:], mode='bilinear', align_corners=True)\n",
    "             \n",
    "        d3 = self.relu3(self.dec_conv3(d3))\n",
    "        \n",
    "        out = self.final(d3)\n",
    "        return out\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    x = torch.randn((2, 3, 560, 560))\n",
    "    f = build_unet()\n",
    "    y = f(x)\n",
    "    print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T10:17:40.437315Z",
     "iopub.status.busy": "2025-12-26T10:17:40.437071Z",
     "iopub.status.idle": "2025-12-26T10:17:40.443522Z",
     "shell.execute_reply": "2025-12-26T10:17:40.442839Z",
     "shell.execute_reply.started": "2025-12-26T10:17:40.437294Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DriveDataset(Dataset):\n",
    "    def __init__(self, images_path, masks_path):\n",
    "\n",
    "        self.images_path = images_path\n",
    "        self.masks_path = masks_path\n",
    "        self.n_samples = len(images_path)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Reading image \"\"\"\n",
    "        image = cv2.imread(self.images_path[index], cv2.IMREAD_COLOR)\n",
    "        image = image/255.0 ## (512, 512, 3)\n",
    "        image = np.transpose(image, (2, 0, 1))  ## (3, 512, 512)\n",
    "        image = image.astype(np.float32)\n",
    "        image = torch.from_numpy(image)\n",
    "\n",
    "        \"\"\" Reading mask \"\"\"\n",
    "        mask = cv2.imread(self.masks_path[index], cv2.IMREAD_GRAYSCALE)\n",
    "        mask = mask/255.0   ## (512, 512)\n",
    "        mask = np.expand_dims(mask, axis=0) ## (1, 512, 512)\n",
    "        mask = mask.astype(np.float32)\n",
    "        mask = torch.from_numpy(mask)\n",
    "        return image, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T10:17:40.445318Z",
     "iopub.status.busy": "2025-12-26T10:17:40.445100Z",
     "iopub.status.idle": "2025-12-26T10:17:40.467440Z",
     "shell.execute_reply": "2025-12-26T10:17:40.466743Z",
     "shell.execute_reply.started": "2025-12-26T10:17:40.445297Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Loss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalTverskyLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal Tversky Loss\n",
    "    特别适合类别极不平衡 + 微结构（血管）\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0.7, beta=0.3, gamma=0.75, smooth=1e-6):\n",
    "        super(FocalTverskyLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        inputs = torch.sigmoid(inputs)\n",
    "\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        TP = (inputs * targets).sum()\n",
    "        FP = ((1 - targets) * inputs).sum()\n",
    "        FN = (targets * (1 - inputs)).sum()\n",
    "\n",
    "        tversky = (TP + self.smooth) / (\n",
    "            TP + self.alpha * FP + self.beta * FN + self.smooth\n",
    "        )\n",
    "\n",
    "        loss = torch.pow((1 - tversky), self.gamma)\n",
    "        return loss\n",
    "\n",
    "class BoundaryLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Boundary Loss via distance transform\n",
    "    强制预测边界贴近 GT 边界\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(BoundaryLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        inputs = torch.sigmoid(inputs)\n",
    "\n",
    "        # 计算边界（简单 Sobel）\n",
    "        def get_boundary(x):\n",
    "            sobel_x = torch.tensor([[1, 0, -1],\n",
    "                                    [2, 0, -2],\n",
    "                                    [1, 0, -1]], device=x.device).float()\n",
    "            sobel_y = sobel_x.t()\n",
    "\n",
    "            sobel_x = sobel_x.view(1, 1, 3, 3)\n",
    "            sobel_y = sobel_y.view(1, 1, 3, 3)\n",
    "\n",
    "            grad_x = F.conv2d(x, sobel_x, padding=1)\n",
    "            grad_y = F.conv2d(x, sobel_y, padding=1)\n",
    "\n",
    "            return torch.sqrt(grad_x ** 2 + grad_y ** 2 + 1e-6)\n",
    "\n",
    "        pred_boundary = get_boundary(inputs)\n",
    "        gt_boundary = get_boundary(targets)\n",
    "\n",
    "        return F.l1_loss(pred_boundary, gt_boundary)\n",
    "\n",
    "class VesselSegmentationLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    最终联合损失：\n",
    "    Focal Tversky + Boundary\n",
    "    \"\"\"\n",
    "    def __init__(self, w_tversky=1.0, w_boundary=0.1):\n",
    "        super(VesselSegmentationLoss, self).__init__()\n",
    "        self.tversky = FocalTverskyLoss()\n",
    "        self.boundary = BoundaryLoss()\n",
    "        self.w_tversky = w_tversky\n",
    "        self.w_boundary = w_boundary\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        loss_t = self.tversky(inputs, targets)\n",
    "        loss_b = self.boundary(inputs, targets)\n",
    "        return self.w_tversky * loss_t + self.w_boundary * loss_b\n",
    "\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "\n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = torch.sigmoid(inputs)\n",
    "\n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n",
    "\n",
    "        return 1 - dice\n",
    "\n",
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "\n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = torch.sigmoid(inputs)\n",
    "\n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n",
    "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
    "        Dice_BCE = BCE + dice_loss\n",
    "\n",
    "        return Dice_BCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T10:17:40.468545Z",
     "iopub.status.busy": "2025-12-26T10:17:40.468339Z",
     "iopub.status.idle": "2025-12-26T10:17:40.482989Z",
     "shell.execute_reply": "2025-12-26T10:17:40.482370Z",
     "shell.execute_reply.started": "2025-12-26T10:17:40.468525Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Utils\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "\"\"\" Seeding the randomness. \"\"\"\n",
    "def seeding(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\"\"\" Create a directory. \"\"\"\n",
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "\"\"\" Calculate the time taken \"\"\"\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T10:17:40.483887Z",
     "iopub.status.busy": "2025-12-26T10:17:40.483664Z",
     "iopub.status.idle": "2025-12-26T10:56:04.787586Z",
     "shell.execute_reply": "2025-12-26T10:56:04.786752Z",
     "shell.execute_reply.started": "2025-12-26T10:17:40.483865Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size:\n",
      "Train: 160 - Valid: 20\n",
      "\n",
      "Valid loss improved from inf to 0.9502. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 01 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.947\n",
      "\t Val. Loss: 0.950\n",
      "\n",
      "Valid loss improved from 0.9502 to 0.9501. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 02 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.947\n",
      "\t Val. Loss: 0.950\n",
      "\n",
      "Valid loss improved from 0.9501 to 0.9497. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 03 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.943\n",
      "\t Val. Loss: 0.950\n",
      "\n",
      "Valid loss improved from 0.9497 to 0.9452. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 04 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.927\n",
      "\t Val. Loss: 0.945\n",
      "\n",
      "Valid loss improved from 0.9452 to 0.9218. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 05 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.911\n",
      "\t Val. Loss: 0.922\n",
      "\n",
      "Valid loss improved from 0.9218 to 0.9098. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 06 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.904\n",
      "\t Val. Loss: 0.910\n",
      "\n",
      "Valid loss improved from 0.9098 to 0.9069. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 07 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.902\n",
      "\t Val. Loss: 0.907\n",
      "\n",
      "Valid loss improved from 0.9069 to 0.9039. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 08 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.902\n",
      "\t Val. Loss: 0.904\n",
      "\n",
      "Valid loss improved from 0.9039 to 0.8992. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 09 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.901\n",
      "\t Val. Loss: 0.899\n",
      "\n",
      "Valid loss improved from 0.8992 to 0.8954. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 10 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.895\n",
      "\t Val. Loss: 0.895\n",
      "\n",
      "Epoch: 11 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.892\n",
      "\t Val. Loss: 0.900\n",
      "\n",
      "Valid loss improved from 0.8954 to 0.8902. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 12 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.846\n",
      "\t Val. Loss: 0.890\n",
      "\n",
      "Valid loss improved from 0.8902 to 0.8859. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 13 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.784\n",
      "\t Val. Loss: 0.886\n",
      "\n",
      "Valid loss improved from 0.8859 to 0.8641. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 14 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.737\n",
      "\t Val. Loss: 0.864\n",
      "\n",
      "Epoch: 15 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.707\n",
      "\t Val. Loss: 0.867\n",
      "\n",
      "Valid loss improved from 0.8641 to 0.8625. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 16 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.672\n",
      "\t Val. Loss: 0.862\n",
      "\n",
      "Valid loss improved from 0.8625 to 0.8244. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 17 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.647\n",
      "\t Val. Loss: 0.824\n",
      "\n",
      "Epoch: 18 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.623\n",
      "\t Val. Loss: 0.867\n",
      "\n",
      "Valid loss improved from 0.8244 to 0.7695. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 19 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.611\n",
      "\t Val. Loss: 0.770\n",
      "\n",
      "Valid loss improved from 0.7695 to 0.7411. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 20 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.593\n",
      "\t Val. Loss: 0.741\n",
      "\n",
      "Valid loss improved from 0.7411 to 0.6453. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 21 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.585\n",
      "\t Val. Loss: 0.645\n",
      "\n",
      "Valid loss improved from 0.6453 to 0.6293. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 22 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.576\n",
      "\t Val. Loss: 0.629\n",
      "\n",
      "Valid loss improved from 0.6293 to 0.5940. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 23 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.569\n",
      "\t Val. Loss: 0.594\n",
      "\n",
      "Valid loss improved from 0.5940 to 0.5820. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 24 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.560\n",
      "\t Val. Loss: 0.582\n",
      "\n",
      "Valid loss improved from 0.5820 to 0.5692. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 25 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.557\n",
      "\t Val. Loss: 0.569\n",
      "\n",
      "Valid loss improved from 0.5692 to 0.5684. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 26 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.548\n",
      "\t Val. Loss: 0.568\n",
      "\n",
      "Valid loss improved from 0.5684 to 0.5367. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 27 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.546\n",
      "\t Val. Loss: 0.537\n",
      "\n",
      "Epoch: 28 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.534\n",
      "\t Val. Loss: 0.538\n",
      "\n",
      "Valid loss improved from 0.5367 to 0.5340. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 29 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.528\n",
      "\t Val. Loss: 0.534\n",
      "\n",
      "Valid loss improved from 0.5340 to 0.5145. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 30 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.518\n",
      "\t Val. Loss: 0.515\n",
      "\n",
      "Epoch: 31 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.505\n",
      "\t Val. Loss: 0.534\n",
      "\n",
      "Valid loss improved from 0.5145 to 0.5038. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 32 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.491\n",
      "\t Val. Loss: 0.504\n",
      "\n",
      "Epoch: 33 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.480\n",
      "\t Val. Loss: 0.528\n",
      "\n",
      "Epoch: 34 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.468\n",
      "\t Val. Loss: 0.508\n",
      "\n",
      "Valid loss improved from 0.5038 to 0.4837. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 35 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.460\n",
      "\t Val. Loss: 0.484\n",
      "\n",
      "Valid loss improved from 0.4837 to 0.4565. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 36 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.449\n",
      "\t Val. Loss: 0.456\n",
      "\n",
      "Valid loss improved from 0.4565 to 0.4419. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 37 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.440\n",
      "\t Val. Loss: 0.442\n",
      "\n",
      "Valid loss improved from 0.4419 to 0.4406. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 38 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.433\n",
      "\t Val. Loss: 0.441\n",
      "\n",
      "Valid loss improved from 0.4406 to 0.4289. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 39 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.431\n",
      "\t Val. Loss: 0.429\n",
      "\n",
      "Valid loss improved from 0.4289 to 0.4140. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 40 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.425\n",
      "\t Val. Loss: 0.414\n",
      "\n",
      "Valid loss improved from 0.4140 to 0.4021. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 41 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.418\n",
      "\t Val. Loss: 0.402\n",
      "\n",
      "Valid loss improved from 0.4021 to 0.4016. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 42 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.416\n",
      "\t Val. Loss: 0.402\n",
      "\n",
      "Valid loss improved from 0.4016 to 0.3945. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 43 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.413\n",
      "\t Val. Loss: 0.395\n",
      "\n",
      "Epoch: 44 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.412\n",
      "\t Val. Loss: 0.396\n",
      "\n",
      "Epoch: 45 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.409\n",
      "\t Val. Loss: 0.398\n",
      "\n",
      "Valid loss improved from 0.3945 to 0.3933. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 46 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.410\n",
      "\t Val. Loss: 0.393\n",
      "\n",
      "Valid loss improved from 0.3933 to 0.3872. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 47 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.403\n",
      "\t Val. Loss: 0.387\n",
      "\n",
      "Epoch: 48 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.402\n",
      "\t Val. Loss: 0.439\n",
      "\n",
      "Epoch: 49 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.399\n",
      "\t Val. Loss: 0.441\n",
      "\n",
      "Epoch: 50 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.400\n",
      "\t Val. Loss: 0.427\n",
      "\n",
      "Valid loss improved from 0.3872 to 0.3822. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 51 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.397\n",
      "\t Val. Loss: 0.382\n",
      "\n",
      "Valid loss improved from 0.3822 to 0.3817. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 52 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.392\n",
      "\t Val. Loss: 0.382\n",
      "\n",
      "Valid loss improved from 0.3817 to 0.3798. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 53 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.389\n",
      "\t Val. Loss: 0.380\n",
      "\n",
      "Valid loss improved from 0.3798 to 0.3749. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 54 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.388\n",
      "\t Val. Loss: 0.375\n",
      "\n",
      "Epoch: 55 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.386\n",
      "\t Val. Loss: 0.388\n",
      "\n",
      "Epoch: 56 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.385\n",
      "\t Val. Loss: 0.383\n",
      "\n",
      "Epoch: 57 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.382\n",
      "\t Val. Loss: 0.381\n",
      "\n",
      "Epoch: 58 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.382\n",
      "\t Val. Loss: 0.377\n",
      "\n",
      "Epoch: 59 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.381\n",
      "\t Val. Loss: 0.390\n",
      "\n",
      "Valid loss improved from 0.3749 to 0.3727. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 60 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.379\n",
      "\t Val. Loss: 0.373\n",
      "\n",
      "Valid loss improved from 0.3727 to 0.3682. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 61 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.380\n",
      "\t Val. Loss: 0.368\n",
      "\n",
      "Valid loss improved from 0.3682 to 0.3675. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 62 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.377\n",
      "\t Val. Loss: 0.367\n",
      "\n",
      "Epoch: 63 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.375\n",
      "\t Val. Loss: 0.370\n",
      "\n",
      "Epoch: 64 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.376\n",
      "\t Val. Loss: 0.369\n",
      "\n",
      "Epoch: 65 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.377\n",
      "\t Val. Loss: 0.369\n",
      "\n",
      "Valid loss improved from 0.3675 to 0.3616. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 66 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.376\n",
      "\t Val. Loss: 0.362\n",
      "\n",
      "Epoch: 67 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.372\n",
      "\t Val. Loss: 0.364\n",
      "\n",
      "Epoch: 68 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.372\n",
      "\t Val. Loss: 0.362\n",
      "\n",
      "Valid loss improved from 0.3616 to 0.3599. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 69 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.373\n",
      "\t Val. Loss: 0.360\n",
      "\n",
      "Epoch: 70 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.371\n",
      "\t Val. Loss: 0.362\n",
      "\n",
      "Epoch: 71 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.373\n",
      "\t Val. Loss: 0.361\n",
      "\n",
      "Epoch: 72 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.371\n",
      "\t Val. Loss: 0.366\n",
      "\n",
      "Epoch: 73 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.374\n",
      "\t Val. Loss: 0.402\n",
      "\n",
      "Epoch: 74 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.372\n",
      "\t Val. Loss: 0.376\n",
      "\n",
      "Epoch: 75 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.369\n",
      "\t Val. Loss: 0.365\n",
      "\n",
      "Epoch: 76 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.369\n",
      "\t Val. Loss: 0.362\n",
      "\n",
      "Valid loss improved from 0.3599 to 0.3550. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 77 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.370\n",
      "\t Val. Loss: 0.355\n",
      "\n",
      "Valid loss improved from 0.3550 to 0.3546. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 78 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.364\n",
      "\t Val. Loss: 0.355\n",
      "\n",
      "Epoch: 79 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.364\n",
      "\t Val. Loss: 0.358\n",
      "\n",
      "Epoch: 80 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.366\n",
      "\t Val. Loss: 0.355\n",
      "\n",
      "Epoch: 81 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.364\n",
      "\t Val. Loss: 0.358\n",
      "\n",
      "Epoch: 82 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.363\n",
      "\t Val. Loss: 0.356\n",
      "\n",
      "Valid loss improved from 0.3546 to 0.3542. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 83 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.364\n",
      "\t Val. Loss: 0.354\n",
      "\n",
      "Epoch: 84 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.362\n",
      "\t Val. Loss: 0.360\n",
      "\n",
      "Valid loss improved from 0.3542 to 0.3534. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 85 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.365\n",
      "\t Val. Loss: 0.353\n",
      "\n",
      "Epoch: 86 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.367\n",
      "\t Val. Loss: 0.365\n",
      "\n",
      "Epoch: 87 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.362\n",
      "\t Val. Loss: 0.365\n",
      "\n",
      "Valid loss improved from 0.3534 to 0.3519. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 88 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.363\n",
      "\t Val. Loss: 0.352\n",
      "\n",
      "Epoch: 89 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.366\n",
      "\t Val. Loss: 0.356\n",
      "\n",
      "Epoch: 90 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.363\n",
      "\t Val. Loss: 0.355\n",
      "\n",
      "Epoch: 91 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.363\n",
      "\t Val. Loss: 0.354\n",
      "\n",
      "Epoch: 92 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.362\n",
      "\t Val. Loss: 0.360\n",
      "\n",
      "Valid loss improved from 0.3519 to 0.3504. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 93 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.360\n",
      "\t Val. Loss: 0.350\n",
      "\n",
      "Epoch: 94 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.360\n",
      "\t Val. Loss: 0.361\n",
      "\n",
      "Epoch: 95 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.357\n",
      "\t Val. Loss: 0.366\n",
      "\n",
      "Valid loss improved from 0.3504 to 0.3503. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 96 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.357\n",
      "\t Val. Loss: 0.350\n",
      "\n",
      "Valid loss improved from 0.3503 to 0.3498. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 97 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.356\n",
      "\t Val. Loss: 0.350\n",
      "\n",
      "Epoch: 98 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.359\n",
      "\t Val. Loss: 0.354\n",
      "\n",
      "Epoch: 99 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.357\n",
      "\t Val. Loss: 0.351\n",
      "\n",
      "Valid loss improved from 0.3498 to 0.3475. Saving checkpoint: working/files/drive_checkpoint_loss.pth\n",
      "Epoch: 100 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.354\n",
      "\t Val. Loss: 0.347\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "import os\n",
    "import time\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "# from data import DriveDataset\n",
    "# from model import build_unet\n",
    "# from loss import DiceLoss, DiceBCELoss\n",
    "# from utils import seeding, create_dir, epoch_time\n",
    "\n",
    "'''训练深度学习模型'''\n",
    "def train(model, loader, optimizer, loss_fn, device, show_images=False):\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    model.train()\n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        x = x.to(device, dtype=torch.float32)\n",
    "        y = y.to(device, dtype=torch.float32)\n",
    "\n",
    "        # if i == 1 and show_images:\n",
    "        #    # 显示第一批图像和掩码\n",
    "        #     img = x[0].cpu().numpy()  # 假设图像是CHW格式\n",
    "        #     img = np.transpose(img, (1, 2, 0))  # 转换为HWC格式\n",
    "        #     img = img[..., ::-1]  # 将BGR转换为RGB\n",
    "        #     mask = y[0].cpu().numpy()  # 假设掩码是CHW格式\n",
    "        #     mask = np.transpose(mask, (1, 2, 0))  # 转换为HWC格式\n",
    "\n",
    "        #     plt.figure(figsize=(12, 6))\n",
    "\n",
    "        #     plt.subplot(1, 2, 1)\n",
    "        #     plt.imshow(img)\n",
    "        #     plt.title(\"Sample Image\")\n",
    "\n",
    "        #     plt.subplot(1, 2, 2)\n",
    "        #     plt.imshow(mask, cmap='gray')\n",
    "        #     plt.title(\"Corresponding Mask\")\n",
    "\n",
    "        #     plt.show()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    #计算整个epoch的平均损失\n",
    "    epoch_loss = epoch_loss/len(loader)\n",
    "    return epoch_loss\n",
    "\n",
    "def evaluate(model, loader, loss_fn, device):\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device, dtype=torch.float32)\n",
    "            y = y.to(device, dtype=torch.float32)\n",
    "\n",
    "            y_pred = model(x)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        epoch_loss = epoch_loss/len(loader)\n",
    "    return epoch_loss\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\" Seeding \"\"\"\n",
    "    seeding(42)\n",
    "\n",
    "    \"\"\" Directories \"\"\"\n",
    "    create_dir(\"files\")\n",
    "\n",
    "    \"\"\" Load dataset \"\"\"\n",
    "    train_x = sorted(glob(\"working/new_data/train/image/*\"))\n",
    "    train_y = sorted(glob(\"working/new_data/train/mask/*\"))\n",
    "\n",
    "    valid_x = sorted(glob(\"working/new_data/test/image/*\"))\n",
    "    valid_y = sorted(glob(\"working/new_data/test/mask/*\"))\n",
    "\n",
    "    data_str = f\"Dataset Size:\\nTrain: {len(train_x)} - Valid: {len(valid_x)}\\n\"\n",
    "    print(data_str)\n",
    "\n",
    "    \"\"\" Hyperparameters \"\"\"\n",
    "    H = 560\n",
    "    W = 560\n",
    "    size = (H, W)\n",
    "    batch_size = 64\n",
    "    num_epochs = 100   \n",
    "    lr = 1e-3\n",
    "    checkpoint_path = \"working/files/drive_checkpoint_loss.pth\"\n",
    "\n",
    "    \"\"\" Dataset and loader \"\"\"\n",
    "    train_dataset = DriveDataset(train_x, train_y)\n",
    "    valid_dataset = DriveDataset(valid_x, valid_y)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        dataset=valid_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   ## GTX 1060 6GB\n",
    "    model = build_unet()\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
    "    # loss_fn = DiceBCELoss()\n",
    "    \n",
    "    loss_fn = VesselSegmentationLoss(\n",
    "        w_tversky=1.0,\n",
    "        w_boundary=0.1\n",
    "    )\n",
    "\n",
    "\n",
    "    \"\"\" Training the model \"\"\"\n",
    "    best_valid_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        #该轮训练的平均损失值 train_loss\n",
    "        train_loss = train(model, train_loader, optimizer, loss_fn, device, show_images=True)\n",
    "        #返回验证损失 valid_loss\n",
    "        valid_loss = evaluate(model, valid_loader, loss_fn, device)\n",
    "\n",
    "        \"\"\" Saving the model \"\"\"\n",
    "        if valid_loss < best_valid_loss:\n",
    "            data_str = f\"Valid loss improved from {best_valid_loss:2.4f} to {valid_loss:2.4f}. Saving checkpoint: {checkpoint_path}\"\n",
    "            print(data_str)\n",
    "\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        data_str = f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\\n'\n",
    "        data_str += f'\\tTrain Loss: {train_loss:.3f}\\n'\n",
    "        data_str += f'\\t Val. Loss: {valid_loss:.3f}\\n'\n",
    "        print(data_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T10:56:04.789919Z",
     "iopub.status.busy": "2025-12-26T10:56:04.789291Z",
     "iopub.status.idle": "2025-12-26T10:56:09.112965Z",
     "shell.execute_reply": "2025-12-26T10:56:09.112196Z",
     "shell.execute_reply.started": "2025-12-26T10:56:04.789886Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1039983/1244756369.py:80: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
      "  0%|          | 0/20 [00:00<?, ?it/s][ WARN:0@336.400] global loadsave.cpp:1063 imwrite_ Unsupported depth image for selected encoder is fallbacked to CV_8U.\n",
      " 15%|█▌        | 3/20 [00:00<00:01, 13.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":-- jaccard:0.6507, f1:0.7884,recall:0.7477,precision:0.834,acc:0.9643,specificity:0.985\n",
      ":-- jaccard:0.6822, f1:0.8111,recall:0.7517,precision:0.881,acc:0.9643,specificity:0.988\n",
      ":-- jaccard:0.4652, f1:0.6350,recall:0.4782,precision:0.945,acc:0.9453,specificity:0.997\n",
      ":-- jaccard:0.6287, f1:0.7720,recall:0.6740,precision:0.903,acc:0.9635,specificity:0.993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [00:00<00:00, 16.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":-- jaccard:0.6258, f1:0.7698,recall:0.6673,precision:0.910,acc:0.9627,specificity:0.993\n",
      ":-- jaccard:0.5739, f1:0.7293,recall:0.6067,precision:0.914,acc:0.9563,specificity:0.994\n",
      ":-- jaccard:0.5717, f1:0.7275,recall:0.6083,precision:0.905,acc:0.9583,specificity:0.994\n",
      ":-- jaccard:0.5328, f1:0.6952,recall:0.5726,precision:0.884,acc:0.9572,specificity:0.993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [00:00<00:00, 17.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":-- jaccard:0.5852, f1:0.7383,recall:0.6409,precision:0.871,acc:0.9632,specificity:0.992\n",
      ":-- jaccard:0.6232, f1:0.7679,recall:0.7059,precision:0.842,acc:0.9652,specificity:0.988\n",
      ":-- jaccard:0.6070, f1:0.7554,recall:0.6679,precision:0.869,acc:0.9617,specificity:0.990\n",
      ":-- jaccard:0.5561, f1:0.7147,recall:0.5828,precision:0.924,acc:0.9601,specificity:0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [00:00<00:00, 18.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":-- jaccard:0.6001, f1:0.7501,recall:0.6473,precision:0.892,acc:0.9578,specificity:0.991\n",
      ":-- jaccard:0.6031, f1:0.7524,recall:0.6641,precision:0.868,acc:0.9647,specificity:0.991\n",
      ":-- jaccard:0.6314, f1:0.7741,recall:0.7252,precision:0.830,acc:0.9698,specificity:0.989\n",
      ":-- jaccard:0.6165, f1:0.7628,recall:0.6837,precision:0.862,acc:0.9616,specificity:0.989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 17.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":-- jaccard:0.5672, f1:0.7238,recall:0.6245,precision:0.861,acc:0.9598,specificity:0.991\n",
      ":-- jaccard:0.6323, f1:0.7748,recall:0.7463,precision:0.805,acc:0.9656,specificity:0.984\n",
      ":-- jaccard:0.6944, f1:0.8197,recall:0.7710,precision:0.875,acc:0.9718,specificity:0.990\n",
      ":-- jaccard:0.6356, f1:0.7772,recall:0.7596,precision:0.796,acc:0.9679,specificity:0.984\n",
      "\n",
      "Overall---Jaccard: 0.6042 - F1: 0.7520 - Sensitivity: 0.6663 - Precision: 0.8735 - Acc: 0.9621 - Specificity:0.9906\n",
      "FPS:  303.0062923069143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "import os, time\n",
    "from operator import add\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import imageio\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, jaccard_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "# from model import build_unet\n",
    "# from utils import create_dir, seeding\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\" Ground truth \"\"\"\n",
    "    y_true = y_true.cpu().numpy()\n",
    "    y_true = y_true > 0.5\n",
    "    y_true = y_true.astype(np.uint8)\n",
    "    y_true = y_true.reshape(-1)\n",
    "\n",
    "    \"\"\" Prediction \"\"\"\n",
    "    y_pred = y_pred.cpu().numpy()\n",
    "    y_pred = y_pred > 0.5\n",
    "    y_pred = y_pred.astype(np.uint8)\n",
    "    y_pred = y_pred.reshape(-1)\n",
    "\n",
    "    # 使用混淆矩阵计算 TP, FP, FN, TN\n",
    "    # labels=[0, 1] 确保即使数据中缺少某一类也能返回 2x2 矩阵\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "\n",
    "    # Dice Coefficient (F1 Score) = 2 * TP / (2 * TP + FP + FN)\n",
    "    score_f1 = 2 * tp / (2 * tp + fp + fn + 1e-6)\n",
    "\n",
    "    # Jaccard (IoU) = TP / (TP + FP + FN)\n",
    "    score_jaccard = tp / (tp + fp + fn + 1e-6)\n",
    "\n",
    "    # Sensitivity (Recall) = TP / (TP + FN)\n",
    "    score_recall = tp / (tp + fn + 1e-6)\n",
    "\n",
    "    # Specificity = TN / (TN + FP)\n",
    "    score_specificity = tn / (tn + fp + 1e-6)\n",
    "\n",
    "    # Precision = TP / (TP + FP)\n",
    "    score_precision = tp / (tp + fp + 1e-6)\n",
    "\n",
    "    # Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    score_acc = (tp + tn) / (tp + tn + fp + fn + 1e-6)\n",
    "\n",
    "    return [score_jaccard, score_f1, score_recall, score_precision, score_acc, score_specificity]\n",
    "\n",
    "def mask_parse(mask):\n",
    "    mask = np.expand_dims(mask, axis=-1)    ## (512, 512, 1)\n",
    "    mask = np.concatenate([mask, mask, mask], axis=-1)  ## (512, 512, 3)\n",
    "    return mask\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\" Seeding \"\"\"\n",
    "    seeding(42)\n",
    "\n",
    "    \"\"\" Folders \"\"\"\n",
    "    create_dir(\"working/results\")\n",
    "\n",
    "    \"\"\" Load dataset \"\"\"\n",
    "    test_x = sorted(glob(\"working/new_data/test/image/*\"))\n",
    "    test_y = sorted(glob(\"working/new_data/test/mask/*\"))\n",
    "\n",
    "    \"\"\" Hyperparameters \"\"\"\n",
    "    H = 560\n",
    "    W = 560\n",
    "    size = (W, H)\n",
    "    checkpoint_path = \"working/files/drive_checkpoint_loss.pth\"\n",
    "\n",
    "    \"\"\" Load the checkpoint \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model = build_unet()\n",
    "    model = model.to(device)\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    metrics_score = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "    time_taken = []\n",
    "    \n",
    "    # Lists to store metrics for each image for plotting\n",
    "    image_indices = []\n",
    "    jaccard_scores = []\n",
    "    f1_scores = []\n",
    "    recall_scores = []\n",
    "    specificity_scores = []\n",
    "\n",
    "    for i, (x, y) in tqdm(enumerate(zip(test_x, test_y)), total=len(test_x)):\n",
    "        \"\"\" Extract the name \"\"\"\n",
    "        name = x.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "        \"\"\" Reading image \"\"\"\n",
    "        image = cv2.imread(x, cv2.IMREAD_COLOR) ## (512, 512, 3)\n",
    "        ## image = cv2.resize(image, size)\n",
    "        x = np.transpose(image, (2, 0, 1))      ## (3, 512, 512)\n",
    "        x = x/255.0\n",
    "        x = np.expand_dims(x, axis=0)           ## (1, 3, 512, 512)\n",
    "        x = x.astype(np.float32)\n",
    "        x = torch.from_numpy(x)\n",
    "        x = x.to(device)\n",
    "\n",
    "        \"\"\" Reading mask \"\"\"\n",
    "        mask = cv2.imread(y, cv2.IMREAD_GRAYSCALE)  ## (512, 512)\n",
    "        ## mask = cv2.resize(mask, size)\n",
    "        y = np.expand_dims(mask, axis=0)            ## (1, 512, 512)\n",
    "        y = y/255.0\n",
    "        y = np.expand_dims(y, axis=0)               ## (1, 1, 512, 512)\n",
    "        y = y.astype(np.float32)\n",
    "        y = torch.from_numpy(y)\n",
    "        y = y.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \"\"\" Prediction and Calculating FPS \"\"\"\n",
    "            start_time = time.time()\n",
    "            pred_y = model(x)\n",
    "            pred_y = torch.sigmoid(pred_y)\n",
    "            total_time = time.time() - start_time\n",
    "            time_taken.append(total_time)\n",
    "\n",
    "\n",
    "            score = calculate_metrics(y, pred_y)\n",
    "            print(f\":-- jaccard:{score[0]:1.4f}, f1:{score[1]:1.4f},recall:{score[2]:1.4f},precision:{score[3]:1.3f},acc:{score[4]:1.4f},specificity:{score[5]:1.3f}\")\n",
    "            \n",
    "            metrics_score = list(map(add, metrics_score, score))\n",
    "            \n",
    "            # Store for plotting\n",
    "            image_indices.append(i)\n",
    "            jaccard_scores.append(score[0])\n",
    "            f1_scores.append(score[1])\n",
    "            recall_scores.append(score[2])\n",
    "            specificity_scores.append(score[5])\n",
    "            \n",
    "            pred_y = pred_y[0].cpu().numpy()        ## (1, 512, 512)\n",
    "            pred_y = np.squeeze(pred_y, axis=0)     ## (512, 512)\n",
    "            pred_y = pred_y > 0.5\n",
    "            pred_y = np.array(pred_y, dtype=np.uint8)\n",
    "\n",
    "        \"\"\" Saving masks \"\"\"\n",
    "        ori_mask = mask_parse(mask)\n",
    "        pred_y = mask_parse(pred_y)\n",
    "        line = np.ones((size[1], 10, 3)) * 128\n",
    "\n",
    "        cat_images = np.concatenate(\n",
    "            [image, line, ori_mask, line, pred_y * 255], axis=1\n",
    "        )\n",
    "        cv2.imwrite(f\"working/results/{name}.png\", cat_images)\n",
    "\n",
    "    jaccard = metrics_score[0]/len(test_x)\n",
    "    f1 = metrics_score[1]/len(test_x)\n",
    "    recall = metrics_score[2]/len(test_x)\n",
    "    precision = metrics_score[3]/len(test_x)\n",
    "    acc = metrics_score[4]/len(test_x)\n",
    "    spec = metrics_score[5]/len(test_x)\n",
    "    print(f\"\\nOverall---Jaccard: {jaccard:1.4f} - F1: {f1:1.4f} - Sensitivity: {recall:1.4f} - Precision: {precision:1.4f} - Acc: {acc:1.4f} - Specificity:{spec:1.4f}\")\n",
    "\n",
    "    fps = 1/np.mean(time_taken)\n",
    "    print(\"FPS: \", fps)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 946814,
     "sourceId": 1604025,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "pampc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
