{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T10:17:27.187106Z",
     "iopub.status.busy": "2025-12-26T10:17:27.186304Z",
     "iopub.status.idle": "2025-12-26T10:17:38.866947Z",
     "shell.execute_reply": "2025-12-26T10:17:38.866281Z",
     "shell.execute_reply.started": "2025-12-26T10:17:27.187068Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangziteng/miniconda3/envs/pampc/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_1006381/3556601113.py:71: UserWarning: Argument(s) 'mode, cval' are not valid for transform Affine\n",
      "  A.Affine(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1\n",
      "Train: 20 - 20\n",
      "Test: 20 - 20\n",
      "Processing Train data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:03<00:00,  6.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 54.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete. The images now contain [Green, CLAHE, TopHat] channels.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Data Augmentation & Preprocessing\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import imageio\n",
    "import albumentations as A  # 确保安装: pip install albumentations\n",
    "\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "\n",
    "\"\"\" 1. 定义预处理类 (特征堆叠的核心) \"\"\"\n",
    "class RetinaPreprocess:\n",
    "    @staticmethod\n",
    "    def get_green_enhanced_stack(img_rgb):\n",
    "        \"\"\"\n",
    "        输入: RGB 图像 (H, W, 3)\n",
    "        输出: 3通道特征图 [Green, CLAHE, TopHat]\n",
    "        \"\"\"\n",
    "        # 1. 提取绿色通道\n",
    "        g, b, r = cv2.split(img_rgb)\n",
    "        green = g # OpenCV读入是BGR, 这里假设输入已经是RGB或者是BGR拆分正确\n",
    "        # 如果输入是RGB: R,G,B = cv2.split(img); green = G\n",
    "        # 为了保险，我们下面统一按 RGB 处理\n",
    "        \n",
    "        # 2. CLAHE (限制对比度自适应直方图均衡化)\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        enhanced_green = clahe.apply(green)\n",
    "        \n",
    "        # 3. Morphological TopHat (形态学顶帽变换 - 提取微血管)\n",
    "        # 血管通常比背景暗，所以用 BlackHat (底帽) 或者反转后的 TopHat\n",
    "        # 这里使用针对血管优化的变体\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "        tophat = cv2.morphologyEx(enhanced_green, cv2.MORPH_TOPHAT, kernel)\n",
    "        \n",
    "        # 也可以尝试 BlackHat 提取暗细节:\n",
    "        # blackhat = cv2.morphologyEx(enhanced_green, cv2.MORPH_BLACKHAT, kernel)\n",
    "        \n",
    "        # 堆叠成 3 通道: [原始Green, 增强Green, 形态学特征]\n",
    "        # 归一化已经在 Dataset 里做了，这里保持 uint8\n",
    "        merged = cv2.merge([green, enhanced_green, tophat])\n",
    "        return merged\n",
    "\n",
    "    @staticmethod\n",
    "    def harmonize_od_color(image, od_mask):\n",
    "        \"\"\"\n",
    "        简单的视盘区域亮度同化，减少视盘边缘的假阳性\n",
    "        \"\"\"\n",
    "        if np.sum(od_mask) == 0:\n",
    "            return image\n",
    "        \n",
    "        # 简单策略：将OD区域的亮度调整为邻域背景亮度 (这里简化为不做处理，防止引入伪影)\n",
    "        # 如果没有完美的 OD Mask，建议跳过此步，避免引入硬边缘噪声\n",
    "        return image\n",
    "\n",
    "\"\"\" 2. 定义增强流水线 (Albumentations) \"\"\"\n",
    "def get_transforms(img_size=560, is_train=True):\n",
    "    if is_train:\n",
    "        return A.Compose([\n",
    "            A.Resize(img_size, img_size),\n",
    "            \n",
    "            # 几何变换\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.RandomRotate90(p=0.5),\n",
    "            \n",
    "            # 仿射变换 (温和)\n",
    "            A.Affine(\n",
    "                scale=(0.95, 1.05),\n",
    "                translate_percent=(0.02, 0.02),\n",
    "                rotate=(-15, 15), # 适度旋转\n",
    "                shear=(-5, 5),    # 适度剪切\n",
    "                mode=cv2.BORDER_CONSTANT,\n",
    "                cval=0,\n",
    "                p=0.5\n",
    "            ),\n",
    "            \n",
    "            # 甚至可以加入弹性变换模拟眼球曲率，但先保持简单\n",
    "            \n",
    "        ], additional_targets={'mask': 'mask'})\n",
    "    else:\n",
    "        # 测试集仅调整大小\n",
    "        return A.Compose([\n",
    "            A.Resize(img_size, img_size)\n",
    "        ])\n",
    "\n",
    "\"\"\" 3. Create Directory \"\"\"\n",
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "''' 4. 加载原始数据路径 '''\n",
    "def load_data(path):\n",
    "    # DRIVE 数据集结构\n",
    "    train_x = sorted(glob(os.path.join(path, \"training\", \"images\", \"*.tif\")))\n",
    "    train_y = sorted(glob(os.path.join(path, \"training\", \"1st_manual\", \"*.gif\")))\n",
    "    \n",
    "    # 尝试加载 OD Mask (如果有的话，没有则忽略)\n",
    "    # train_od = sorted(glob(os.path.join(path, \"training\", \"mask\", \"*.gif\"))) # 假设路径\n",
    "\n",
    "    test_x = sorted(glob(os.path.join(path, \"test\", \"images\", \"*.tif\")))\n",
    "    test_y = sorted(glob(os.path.join(path, \"test\", \"1st_manual\", \"*.gif\")))\n",
    "\n",
    "    return (train_x, train_y), (test_x, test_y)\n",
    "\n",
    "''' 5. 核心处理函数 '''\n",
    "def process_and_augment(images, masks, save_path, augment_times=1, is_train=True):\n",
    "    \"\"\"\n",
    "    augment_times: 每张图扩充多少倍 (仅对训练集有效)\n",
    "    is_train: 是否为训练集 (决定是否应用几何增强)\n",
    "    \"\"\"\n",
    "    transform = get_transforms(img_size=560, is_train=is_train)\n",
    "    \n",
    "    print(f\"Processing {'Train' if is_train else 'Test'} data...\")\n",
    "    \n",
    "    for idx, (x_path, y_path) in tqdm(enumerate(zip(images, masks)), total=len(images)):\n",
    "        name = x_path.split(os.sep)[-1].split(\".\")[0]\n",
    "        \n",
    "        # 读取\n",
    "        # OpenCV 读入是 BGR\n",
    "        img_bgr = cv2.imread(x_path, cv2.IMREAD_COLOR) \n",
    "        mask = imageio.mimread(y_path)[0]\n",
    "\n",
    "        if img_bgr is None or mask is None:\n",
    "            continue\n",
    "\n",
    "        # 转 RGB\n",
    "        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # === 核心步骤 1: 特征工程预处理 ===\n",
    "        # 注意：无论是训练集还是测试集，都必须做这一步！\n",
    "        # 这样网络学到的是 \"Green+CLAHE+TopHat\" 的特征，测试时也必须给它这个特征。\n",
    "        feature_stack = RetinaPreprocess.get_green_enhanced_stack(img_rgb)\n",
    "        \n",
    "        # 准备增强循环\n",
    "        # 如果是测试集，augment_times 强制为 1\n",
    "        loops = augment_times if is_train else 1\n",
    "        \n",
    "        for i in range(loops):\n",
    "            # === 核心步骤 2: 几何增强 (Albumentations) ===\n",
    "            augmented = transform(image=feature_stack, mask=mask)\n",
    "            \n",
    "            aug_img = augmented['image']\n",
    "            aug_mask = augmented['mask']\n",
    "            \n",
    "            # 命名\n",
    "            if is_train:\n",
    "                tmp_image_name = f\"{name}_{i}.png\"\n",
    "                tmp_mask_name = f\"{name}_{i}.png\"\n",
    "            else:\n",
    "                # 测试集保持原名，方便对应\n",
    "                tmp_image_name = f\"{name}.png\"\n",
    "                tmp_mask_name = f\"{name}.png\"\n",
    "\n",
    "            image_save_path = os.path.join(save_path, \"image\", tmp_image_name)\n",
    "            mask_save_path = os.path.join(save_path, \"mask\", tmp_mask_name)\n",
    "\n",
    "            # 保存\n",
    "            # 注意：cv2.imwrite 期望 BGR 顺序。\n",
    "            # 我们的 feature_stack 是 [Green, CLAHE, TopHat]。\n",
    "            # 保存后，读取时 cv2.imread 也会按 [ch0, ch1, ch2] 读入，顺序一致，所以直接写即可。\n",
    "            cv2.imwrite(image_save_path, aug_img)\n",
    "            cv2.imwrite(mask_save_path, aug_mask)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\" Seeding \"\"\"\n",
    "    np.random.seed(42)\n",
    "\n",
    "    \"\"\" Load the data \"\"\"\n",
    "    data_path = \"data/DRIVE/\" # 请确保路径正确\n",
    "    \n",
    "    if os.path.exists(data_path):\n",
    "        (train_x, train_y), (test_x, test_y) = load_data(data_path)\n",
    "\n",
    "        print(f\"Train: {len(train_x)} - {len(train_y)}\")\n",
    "        print(f\"Test: {len(test_x)} - {len(test_y)}\")\n",
    "\n",
    "        \"\"\" Create directories \"\"\"\n",
    "        # 建议清理旧数据，防止混淆\n",
    "        # import shutil\n",
    "        # shutil.rmtree(\"working/new_data\", ignore_errors=True)\n",
    "        \n",
    "        create_dir(\"working/new_data/train/image/\")\n",
    "        create_dir(\"working/new_data/train/mask/\")\n",
    "        create_dir(\"working/new_data/test/image/\")\n",
    "        create_dir(\"working/new_data/test/mask/\")\n",
    "\n",
    "        \"\"\" Run Processing \"\"\"\n",
    "        # 训练集：特征工程 + 10倍几何增强\n",
    "        process_and_augment(train_x, train_y, \"working/new_data/train/\", augment_times=12, is_train=True)\n",
    "        \n",
    "        # 测试集：仅特征工程 + Resize (无几何增强)\n",
    "        process_and_augment(test_x, test_y, \"working/new_data/test/\", augment_times=1, is_train=False)\n",
    "        \n",
    "        print(\"Data processing complete. The images now contain [Green, CLAHE, TopHat] channels.\")\n",
    "    else:\n",
    "        print(f\"Data path not found: {data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T10:17:38.868551Z",
     "iopub.status.busy": "2025-12-26T10:17:38.868200Z",
     "iopub.status.idle": "2025-12-26T10:17:40.435461Z",
     "shell.execute_reply": "2025-12-26T10:17:40.434747Z",
     "shell.execute_reply.started": "2025-12-26T10:17:38.868526Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 560, 560])\n"
     ]
    }
   ],
   "source": [
    "# Model (LFA-Net)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalModulation(nn.Module):\n",
    "    def __init__(self, in_channels, gamma=2.0, alpha=0.25):\n",
    "        super(FocalModulation, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.gmp = nn.AdaptiveMaxPool2d(1)\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = self.gap(x)\n",
    "        max_val = self.gmp(x)\n",
    "        modulation = (max_val - mean) * self.alpha\n",
    "        modulation = self.conv(modulation)\n",
    "        modulation = self.sigmoid(modulation)\n",
    "        scaled_inputs = x * modulation\n",
    "        outputs = torch.pow(scaled_inputs, self.gamma)\n",
    "        return outputs\n",
    "\n",
    "class FocalModulationContextAggregation(nn.Module):\n",
    "    def __init__(self, in_channels, filters):\n",
    "        super(FocalModulationContextAggregation, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, filters, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(in_channels, filters, kernel_size=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv_ctx = nn.Conv2d(filters, filters, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.focal_mod = FocalModulation(filters)\n",
    "\n",
    "    def forward(self, x):\n",
    "        c1 = self.relu1(self.conv1(x))\n",
    "        c2 = self.relu2(self.conv2(x))\n",
    "        \n",
    "        global_context = self.gap(c2)\n",
    "        global_context = self.sigmoid(self.conv_ctx(global_context))\n",
    "        global_context = c1 * global_context\n",
    "        \n",
    "        fm = self.focal_mod(global_context)\n",
    "        return torch.cat([c1, fm], dim=1)\n",
    "\n",
    "class VisionMambaInspired(nn.Module):\n",
    "    def __init__(self, dim, dropout_rate=0.1):\n",
    "        super(VisionMambaInspired, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.token_mixer = nn.Sequential(\n",
    "            nn.Conv2d(dim, dim, kernel_size=3, padding=1, groups=dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.channel_mixer = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(dim * 4, dim),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        \n",
    "        shortcut = x\n",
    "        x_perm = x.permute(0, 2, 3, 1)\n",
    "        x_norm = self.norm1(x_perm).permute(0, 3, 1, 2)\n",
    "        x_tm = self.token_mixer(x_norm) + shortcut\n",
    "        \n",
    "        shortcut = x_tm\n",
    "        x_perm = x_tm.permute(0, 2, 3, 1)\n",
    "        x_norm = self.norm2(x_perm)\n",
    "        x_cm = self.channel_mixer(x_norm)\n",
    "        x_cm = x_cm.permute(0, 3, 1, 2)\n",
    "        \n",
    "        return x_cm + shortcut\n",
    "\n",
    "class LiteFusionAttention(nn.Module):\n",
    "    def __init__(self, in_channels, filters):\n",
    "        super(LiteFusionAttention, self).__init__()\n",
    "        self.proj1 = nn.Conv2d(in_channels, filters, kernel_size=1)\n",
    "        self.norm = nn.LayerNorm(filters)\n",
    "        self.conv = nn.Conv2d(filters, filters, kernel_size=3, padding=1)\n",
    "        self.fmca = FocalModulationContextAggregation(filters, filters)\n",
    "        self.proj2 = nn.Conv2d(2 * filters, filters, kernel_size=1)\n",
    "        self.vm = VisionMambaInspired(filters)\n",
    "        \n",
    "        self.res_proj = nn.Conv2d(in_channels, filters, kernel_size=1) if in_channels != filters else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_tensor = self.proj1(x)\n",
    "        \n",
    "        x_perm = input_tensor.permute(0, 2, 3, 1)\n",
    "        x_norm = self.norm(x_perm).permute(0, 3, 1, 2)\n",
    "        \n",
    "        x_conv = self.conv(x_norm)\n",
    "        x_fmca = self.fmca(x_conv)\n",
    "        x_proj = self.proj2(x_fmca)\n",
    "        \n",
    "        res = self.res_proj(x) if isinstance(self.res_proj, nn.Conv2d) else input_tensor\n",
    "        out = x_proj + res\n",
    "        \n",
    "        out = self.vm(out)\n",
    "        return out\n",
    "\n",
    "class RA_AttentionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, n_classes, k):\n",
    "        super(RA_AttentionBlock, self).__init__()\n",
    "        self.k = k\n",
    "        self.n_classes = n_classes\n",
    "        self.conv = nn.Conv2d(in_channels, k * n_classes, kernel_size=3, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(k * n_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.gmp = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        f = self.relu(self.bn(self.conv(x)))\n",
    "        \n",
    "        x1 = self.gmp(f)\n",
    "        x2 = self.gap(f)\n",
    "        x_mul = x1 * x2\n",
    "        \n",
    "        x_reshape = x_mul.view(b, self.n_classes, self.k)\n",
    "        s = torch.mean(x_reshape, dim=-1, keepdim=False)\n",
    "        \n",
    "        f_perm = f.permute(0, 2, 3, 1)\n",
    "        f_reshape = f_perm.view(b, h, w, self.n_classes, self.k)\n",
    "        f_mean = torch.mean(f_reshape, dim=-1, keepdim=False)\n",
    "        \n",
    "        s_expanded = s.view(b, 1, 1, self.n_classes)\n",
    "        x_weighted = f_mean * s_expanded\n",
    "        \n",
    "        m = torch.mean(x_weighted, dim=-1, keepdim=True)\n",
    "        m = m.permute(0, 3, 1, 2)\n",
    "        \n",
    "        semantic = x * m\n",
    "        return semantic\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout_rate=0.5):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1x1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0, bias=False)\n",
    "        self.conv3x3 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.conv3x3_dilated = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=2, dilation=2, bias=False)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        c1 = self.conv1x1(x)\n",
    "        c3 = self.conv3x3(x)\n",
    "        c3d = self.conv3x3_dilated(x)\n",
    "        out = c1 + c3 + c3d\n",
    "        out = self.leaky_relu(out)\n",
    "        return out\n",
    "\n",
    "class build_unet(nn.Module):\n",
    "    def __init__(self, input_channels=3, num_classes=1, feature_scale=2, dropout=0.5):\n",
    "        super(build_unet, self).__init__()\n",
    "        filters = [int(x / feature_scale) for x in [16, 32, 64]]\n",
    "        \n",
    "        self.conv1 = ConvBlock(input_channels, filters[0], dropout)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.bn1 = nn.BatchNorm2d(filters[0])\n",
    "        \n",
    "        self.conv2 = ConvBlock(filters[0], filters[1], dropout)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.bn2 = nn.BatchNorm2d(filters[1])\n",
    "        \n",
    "        self.conv3 = ConvBlock(filters[1], filters[2], dropout)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.bn3 = nn.BatchNorm2d(filters[2])\n",
    "        \n",
    "        self.lfa = LiteFusionAttention(filters[2], filters=32)\n",
    "        \n",
    "        lfa_out_channels = 32 \n",
    "        self.att1 = RA_AttentionBlock(lfa_out_channels, 1, 16)\n",
    "        \n",
    "        self.up1 = nn.ConvTranspose2d(lfa_out_channels * 2, filters[2], kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        \n",
    "        self.att2 = RA_AttentionBlock(filters[1], 1, 16)\n",
    "        self.dec_conv1 = nn.Conv2d(filters[1] + filters[2], filters[2], kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.up2 = nn.ConvTranspose2d(filters[2], filters[2], kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        \n",
    "        self.att3 = RA_AttentionBlock(filters[0], 1, 16)\n",
    "        self.dec_conv2 = nn.Conv2d(filters[0] + filters[2], filters[2], kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.up3 = nn.ConvTranspose2d(filters[2], filters[0], kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.dec_conv3 = nn.Conv2d(filters[0], filters[0], kernel_size=3, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.final = nn.Conv2d(filters[0], num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        c1 = self.conv1(x)\n",
    "        p1 = self.bn1(self.pool1(c1))\n",
    "        \n",
    "        c2 = self.conv2(p1)\n",
    "        p2 = self.bn2(self.pool2(c2))\n",
    "        \n",
    "        c3 = self.conv3(p2)\n",
    "        p3 = self.bn3(self.pool3(c3))\n",
    "        \n",
    "        lfa = self.lfa(p3)\n",
    "        \n",
    "        att1 = self.att1(lfa)\n",
    "        fused = torch.cat([att1, lfa], dim=1)\n",
    "        \n",
    "        d1 = self.up1(fused)\n",
    "        if d1.size() != c2.size():\n",
    "             d1 = F.interpolate(d1, size=c2.shape[2:], mode='bilinear', align_corners=True)\n",
    "             \n",
    "        att2 = self.att2(c2)\n",
    "        d1 = torch.cat([att2, d1], dim=1)\n",
    "        d1 = self.relu1(self.dec_conv1(d1))\n",
    "        \n",
    "        d2 = self.up2(d1)\n",
    "        if d2.size() != c1.size():\n",
    "             d2 = F.interpolate(d2, size=c1.shape[2:], mode='bilinear', align_corners=True)\n",
    "\n",
    "        att3 = self.att3(c1)\n",
    "        d2 = torch.cat([att3, d2], dim=1)\n",
    "        d2 = self.relu2(self.dec_conv2(d2))\n",
    "        \n",
    "        d3 = self.up3(d2)\n",
    "        if d3.size() != x.size():\n",
    "             d3 = F.interpolate(d3, size=x.shape[2:], mode='bilinear', align_corners=True)\n",
    "             \n",
    "        d3 = self.relu3(self.dec_conv3(d3))\n",
    "        \n",
    "        out = self.final(d3)\n",
    "        return out\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    x = torch.randn((2, 3, 560, 560))\n",
    "    f = build_unet()\n",
    "    y = f(x)\n",
    "    print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T10:17:40.437315Z",
     "iopub.status.busy": "2025-12-26T10:17:40.437071Z",
     "iopub.status.idle": "2025-12-26T10:17:40.443522Z",
     "shell.execute_reply": "2025-12-26T10:17:40.442839Z",
     "shell.execute_reply.started": "2025-12-26T10:17:40.437294Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DriveDataset(Dataset):\n",
    "    def __init__(self, images_path, masks_path):\n",
    "\n",
    "        self.images_path = images_path\n",
    "        self.masks_path = masks_path\n",
    "        self.n_samples = len(images_path)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Reading image \"\"\"\n",
    "        image = cv2.imread(self.images_path[index], cv2.IMREAD_COLOR)\n",
    "        image = image/255.0 ## (512, 512, 3)\n",
    "        image = np.transpose(image, (2, 0, 1))  ## (3, 512, 512)\n",
    "        image = image.astype(np.float32)\n",
    "        image = torch.from_numpy(image)\n",
    "\n",
    "        \"\"\" Reading mask \"\"\"\n",
    "        mask = cv2.imread(self.masks_path[index], cv2.IMREAD_GRAYSCALE)\n",
    "        mask = mask/255.0   ## (512, 512)\n",
    "        mask = np.expand_dims(mask, axis=0) ## (1, 512, 512)\n",
    "        mask = mask.astype(np.float32)\n",
    "        mask = torch.from_numpy(mask)\n",
    "        return image, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T10:17:40.445318Z",
     "iopub.status.busy": "2025-12-26T10:17:40.445100Z",
     "iopub.status.idle": "2025-12-26T10:17:40.467440Z",
     "shell.execute_reply": "2025-12-26T10:17:40.466743Z",
     "shell.execute_reply.started": "2025-12-26T10:17:40.445297Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Loss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "\n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = torch.sigmoid(inputs)\n",
    "\n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n",
    "\n",
    "        return 1 - dice\n",
    "\n",
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "\n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = torch.sigmoid(inputs)\n",
    "\n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n",
    "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
    "        Dice_BCE = BCE + dice_loss\n",
    "\n",
    "        return Dice_BCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T10:17:40.468545Z",
     "iopub.status.busy": "2025-12-26T10:17:40.468339Z",
     "iopub.status.idle": "2025-12-26T10:17:40.482989Z",
     "shell.execute_reply": "2025-12-26T10:17:40.482370Z",
     "shell.execute_reply.started": "2025-12-26T10:17:40.468525Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Utils\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "\"\"\" Seeding the randomness. \"\"\"\n",
    "def seeding(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\"\"\" Create a directory. \"\"\"\n",
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "\"\"\" Calculate the time taken \"\"\"\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T10:17:40.483887Z",
     "iopub.status.busy": "2025-12-26T10:17:40.483664Z",
     "iopub.status.idle": "2025-12-26T10:56:04.787586Z",
     "shell.execute_reply": "2025-12-26T10:56:04.786752Z",
     "shell.execute_reply.started": "2025-12-26T10:17:40.483865Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size:\n",
      "Train: 240 - Valid: 20\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss improved from inf to 1.5080. Saving checkpoint: working/files/drive_checkpoint_ori.pth\n",
      "Epoch: 01 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 1.518\n",
      "\t Val. Loss: 1.508\n",
      "\n",
      "Valid loss improved from 1.5080 to 1.3882. Saving checkpoint: working/files/drive_checkpoint_ori.pth\n",
      "Epoch: 02 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 1.458\n",
      "\t Val. Loss: 1.388\n",
      "\n",
      "Valid loss improved from 1.3882 to 1.2473. Saving checkpoint: working/files/drive_checkpoint_ori.pth\n",
      "Epoch: 03 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 1.240\n",
      "\t Val. Loss: 1.247\n",
      "\n",
      "Epoch: 04 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 1.120\n",
      "\t Val. Loss: 1.250\n",
      "\n",
      "Epoch: 05 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 1.092\n",
      "\t Val. Loss: 1.494\n",
      "\n",
      "Epoch: 06 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 1.067\n",
      "\t Val. Loss: 1.578\n",
      "\n",
      "Epoch: 07 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 1.051\n",
      "\t Val. Loss: 1.500\n",
      "\n",
      "Epoch: 08 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 1.032\n",
      "\t Val. Loss: 1.348\n",
      "\n",
      "Epoch: 09 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 1.013\n",
      "\t Val. Loss: 1.277\n",
      "\n",
      "Epoch: 10 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.991\n",
      "\t Val. Loss: 1.298\n",
      "\n",
      "Epoch: 11 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.966\n",
      "\t Val. Loss: 1.349\n",
      "\n",
      "Epoch: 12 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.947\n",
      "\t Val. Loss: 1.253\n",
      "\n",
      "Valid loss improved from 1.2473 to 1.1872. Saving checkpoint: working/files/drive_checkpoint_ori.pth\n",
      "Epoch: 13 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.923\n",
      "\t Val. Loss: 1.187\n",
      "\n",
      "Valid loss improved from 1.1872 to 1.1323. Saving checkpoint: working/files/drive_checkpoint_ori.pth\n",
      "Epoch: 14 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.884\n",
      "\t Val. Loss: 1.132\n",
      "\n",
      "Epoch: 15 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.839\n",
      "\t Val. Loss: 1.161\n",
      "\n",
      "Valid loss improved from 1.1323 to 1.0086. Saving checkpoint: working/files/drive_checkpoint_ori.pth\n",
      "Epoch: 16 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.814\n",
      "\t Val. Loss: 1.009\n",
      "\n",
      "Valid loss improved from 1.0086 to 0.9147. Saving checkpoint: working/files/drive_checkpoint_ori.pth\n",
      "Epoch: 17 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.797\n",
      "\t Val. Loss: 0.915\n",
      "\n",
      "Valid loss improved from 0.9147 to 0.8684. Saving checkpoint: working/files/drive_checkpoint_ori.pth\n",
      "Epoch: 18 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.785\n",
      "\t Val. Loss: 0.868\n",
      "\n",
      "Valid loss improved from 0.8684 to 0.8058. Saving checkpoint: working/files/drive_checkpoint_ori.pth\n",
      "Epoch: 19 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.773\n",
      "\t Val. Loss: 0.806\n",
      "\n",
      "Valid loss improved from 0.8058 to 0.7507. Saving checkpoint: working/files/drive_checkpoint_ori.pth\n",
      "Epoch: 20 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.760\n",
      "\t Val. Loss: 0.751\n",
      "\n",
      "Valid loss improved from 0.7507 to 0.7165. Saving checkpoint: working/files/drive_checkpoint_ori.pth\n",
      "Epoch: 21 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.750\n",
      "\t Val. Loss: 0.717\n",
      "\n",
      "Valid loss improved from 0.7165 to 0.7108. Saving checkpoint: working/files/drive_checkpoint_ori.pth\n",
      "Epoch: 22 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.743\n",
      "\t Val. Loss: 0.711\n",
      "\n",
      "Valid loss improved from 0.7108 to 0.7046. Saving checkpoint: working/files/drive_checkpoint_ori.pth\n",
      "Epoch: 23 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.735\n",
      "\t Val. Loss: 0.705\n",
      "\n",
      "Valid loss improved from 0.7046 to 0.6838. Saving checkpoint: working/files/drive_checkpoint_ori.pth\n",
      "Epoch: 24 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.719\n",
      "\t Val. Loss: 0.684\n",
      "\n",
      "Valid loss improved from 0.6838 to 0.6769. Saving checkpoint: working/files/drive_checkpoint_ori.pth\n",
      "Epoch: 25 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.710\n",
      "\t Val. Loss: 0.677\n",
      "\n",
      "Valid loss improved from 0.6769 to 0.6582. Saving checkpoint: working/files/drive_checkpoint_ori.pth\n",
      "Epoch: 26 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.701\n",
      "\t Val. Loss: 0.658\n",
      "\n",
      "Epoch: 27 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.691\n",
      "\t Val. Loss: 0.703\n",
      "\n",
      "Epoch: 28 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.691\n",
      "\t Val. Loss: 0.658\n",
      "\n",
      "Valid loss improved from 0.6582 to 0.6430. Saving checkpoint: working/files/drive_checkpoint_ori.pth\n",
      "Epoch: 29 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.682\n",
      "\t Val. Loss: 0.643\n",
      "\n",
      "Valid loss improved from 0.6430 to 0.6419. Saving checkpoint: working/files/drive_checkpoint_ori.pth\n",
      "Epoch: 30 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.674\n",
      "\t Val. Loss: 0.642\n",
      "\n",
      "Valid loss improved from 0.6419 to 0.6304. Saving checkpoint: working/files/drive_checkpoint_ori.pth\n",
      "Epoch: 31 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.663\n",
      "\t Val. Loss: 0.630\n",
      "\n",
      "Epoch: 32 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.660\n",
      "\t Val. Loss: 0.666\n",
      "\n",
      "Epoch: 33 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.670\n",
      "\t Val. Loss: 0.686\n",
      "\n",
      "Epoch: 34 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.658\n",
      "\t Val. Loss: 0.684\n",
      "\n",
      "Epoch: 35 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.649\n",
      "\t Val. Loss: 0.635\n",
      "\n",
      "Epoch: 36 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.654\n",
      "\t Val. Loss: 0.638\n",
      "\n",
      "Epoch: 37 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.656\n",
      "\t Val. Loss: 0.746\n",
      "\n",
      "Epoch: 38 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.651\n",
      "\t Val. Loss: 0.669\n",
      "\n",
      "Valid loss improved from 0.6304 to 0.6297. Saving checkpoint: working/files/drive_checkpoint_ori.pth\n",
      "Epoch: 39 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.638\n",
      "\t Val. Loss: 0.630\n",
      "\n",
      "Valid loss improved from 0.6297 to 0.6067. Saving checkpoint: working/files/drive_checkpoint_ori.pth\n",
      "Epoch: 40 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.632\n",
      "\t Val. Loss: 0.607\n",
      "\n",
      "Epoch: 41 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.627\n",
      "\t Val. Loss: 0.610\n",
      "\n",
      "Valid loss improved from 0.6067 to 0.5990. Saving checkpoint: working/files/drive_checkpoint_ori.pth\n",
      "Epoch: 42 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.623\n",
      "\t Val. Loss: 0.599\n",
      "\n",
      "Valid loss improved from 0.5990 to 0.5949. Saving checkpoint: working/files/drive_checkpoint_ori.pth\n",
      "Epoch: 43 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.620\n",
      "\t Val. Loss: 0.595\n",
      "\n",
      "Epoch: 44 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.618\n",
      "\t Val. Loss: 0.609\n",
      "\n",
      "Valid loss improved from 0.5949 to 0.5753. Saving checkpoint: working/files/drive_checkpoint_ori.pth\n",
      "Epoch: 45 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.617\n",
      "\t Val. Loss: 0.575\n",
      "\n",
      "Epoch: 46 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.610\n",
      "\t Val. Loss: 0.627\n",
      "\n",
      "Epoch: 47 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.610\n",
      "\t Val. Loss: 0.594\n",
      "\n",
      "Valid loss improved from 0.5753 to 0.5743. Saving checkpoint: working/files/drive_checkpoint_ori.pth\n",
      "Epoch: 48 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.604\n",
      "\t Val. Loss: 0.574\n",
      "\n",
      "Epoch: 49 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.603\n",
      "\t Val. Loss: 0.593\n",
      "\n",
      "Epoch: 50 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.600\n",
      "\t Val. Loss: 0.575\n",
      "\n",
      "Epoch: 51 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.598\n",
      "\t Val. Loss: 0.591\n",
      "\n",
      "Epoch: 52 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.598\n",
      "\t Val. Loss: 0.577\n",
      "\n",
      "Epoch: 53 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.599\n",
      "\t Val. Loss: 0.586\n",
      "\n",
      "Valid loss improved from 0.5743 to 0.5683. Saving checkpoint: working/files/drive_checkpoint_ori.pth\n",
      "Epoch: 54 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.599\n",
      "\t Val. Loss: 0.568\n",
      "\n",
      "Epoch: 55 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.594\n",
      "\t Val. Loss: 0.574\n",
      "\n",
      "Valid loss improved from 0.5683 to 0.5637. Saving checkpoint: working/files/drive_checkpoint_ori.pth\n",
      "Epoch: 56 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.591\n",
      "\t Val. Loss: 0.564\n",
      "\n",
      "Epoch: 57 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.586\n",
      "\t Val. Loss: 0.572\n",
      "\n",
      "Epoch: 58 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.584\n",
      "\t Val. Loss: 0.570\n",
      "\n",
      "Valid loss improved from 0.5637 to 0.5585. Saving checkpoint: working/files/drive_checkpoint_ori.pth\n",
      "Epoch: 59 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.583\n",
      "\t Val. Loss: 0.559\n",
      "\n",
      "Epoch: 60 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.581\n",
      "\t Val. Loss: 0.571\n",
      "\n",
      "Valid loss improved from 0.5585 to 0.5554. Saving checkpoint: working/files/drive_checkpoint_ori.pth\n",
      "Epoch: 61 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.576\n",
      "\t Val. Loss: 0.555\n",
      "\n",
      "Epoch: 62 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.576\n",
      "\t Val. Loss: 0.571\n",
      "\n",
      "Epoch: 63 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.574\n",
      "\t Val. Loss: 0.575\n",
      "\n",
      "Epoch: 64 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.580\n",
      "\t Val. Loss: 0.603\n",
      "\n",
      "Valid loss improved from 0.5554 to 0.5527. Saving checkpoint: working/files/drive_checkpoint_ori.pth\n",
      "Epoch: 65 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.577\n",
      "\t Val. Loss: 0.553\n",
      "\n",
      "Epoch: 66 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.576\n",
      "\t Val. Loss: 0.579\n",
      "\n",
      "Epoch: 67 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.573\n",
      "\t Val. Loss: 0.571\n",
      "\n",
      "Epoch: 68 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.582\n",
      "\t Val. Loss: 0.571\n",
      "\n",
      "Epoch: 69 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.573\n",
      "\t Val. Loss: 0.567\n",
      "\n",
      "Valid loss improved from 0.5527 to 0.5435. Saving checkpoint: working/files/drive_checkpoint_ori.pth\n",
      "Epoch: 70 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.570\n",
      "\t Val. Loss: 0.543\n",
      "\n",
      "Epoch: 71 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.567\n",
      "\t Val. Loss: 0.603\n",
      "\n",
      "Epoch: 72 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.567\n",
      "\t Val. Loss: 0.565\n",
      "\n",
      "Epoch: 73 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.564\n",
      "\t Val. Loss: 0.560\n",
      "\n",
      "Epoch: 74 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.560\n",
      "\t Val. Loss: 0.560\n",
      "\n",
      "Epoch: 75 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.560\n",
      "\t Val. Loss: 0.557\n",
      "\n",
      "Valid loss improved from 0.5435 to 0.5399. Saving checkpoint: working/files/drive_checkpoint_ori.pth\n",
      "Epoch: 76 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.559\n",
      "\t Val. Loss: 0.540\n",
      "\n",
      "Epoch: 77 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.561\n",
      "\t Val. Loss: 0.542\n",
      "\n",
      "Valid loss improved from 0.5399 to 0.5363. Saving checkpoint: working/files/drive_checkpoint_ori.pth\n",
      "Epoch: 78 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.559\n",
      "\t Val. Loss: 0.536\n",
      "\n",
      "Epoch: 79 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.557\n",
      "\t Val. Loss: 0.579\n",
      "\n",
      "Epoch: 80 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.554\n",
      "\t Val. Loss: 0.537\n",
      "\n",
      "Valid loss improved from 0.5363 to 0.5322. Saving checkpoint: working/files/drive_checkpoint_ori.pth\n",
      "Epoch: 81 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.552\n",
      "\t Val. Loss: 0.532\n",
      "\n",
      "Epoch: 82 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.554\n",
      "\t Val. Loss: 0.638\n",
      "\n",
      "Epoch: 83 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.558\n",
      "\t Val. Loss: 0.550\n",
      "\n",
      "Valid loss improved from 0.5322 to 0.5281. Saving checkpoint: working/files/drive_checkpoint_ori.pth\n",
      "Epoch: 84 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.555\n",
      "\t Val. Loss: 0.528\n",
      "\n",
      "Epoch: 85 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.549\n",
      "\t Val. Loss: 0.532\n",
      "\n",
      "Epoch: 86 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.546\n",
      "\t Val. Loss: 0.539\n",
      "\n",
      "Valid loss improved from 0.5281 to 0.5161. Saving checkpoint: working/files/drive_checkpoint_ori.pth\n",
      "Epoch: 87 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.544\n",
      "\t Val. Loss: 0.516\n",
      "\n",
      "Epoch: 88 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.547\n",
      "\t Val. Loss: 0.519\n",
      "\n",
      "Epoch: 89 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.543\n",
      "\t Val. Loss: 0.524\n",
      "\n",
      "Epoch: 90 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.544\n",
      "\t Val. Loss: 0.521\n",
      "\n",
      "Epoch: 91 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.540\n",
      "\t Val. Loss: 0.533\n",
      "\n",
      "Epoch: 92 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.539\n",
      "\t Val. Loss: 0.526\n",
      "\n",
      "Epoch: 93 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.539\n",
      "\t Val. Loss: 0.522\n",
      "\n",
      "Epoch: 94 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.539\n",
      "\t Val. Loss: 0.526\n",
      "\n",
      "Valid loss improved from 0.5161 to 0.5071. Saving checkpoint: working/files/drive_checkpoint_ori.pth\n",
      "Epoch: 95 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.537\n",
      "\t Val. Loss: 0.507\n",
      "\n",
      "Epoch: 96 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.537\n",
      "\t Val. Loss: 0.525\n",
      "\n",
      "Epoch: 97 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.536\n",
      "\t Val. Loss: 0.511\n",
      "\n",
      "Epoch: 98 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.533\n",
      "\t Val. Loss: 0.520\n",
      "\n",
      "Epoch: 99 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.536\n",
      "\t Val. Loss: 0.517\n",
      "\n",
      "Epoch: 100 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.536\n",
      "\t Val. Loss: 0.512\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "import os\n",
    "import time\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "# from data import DriveDataset\n",
    "# from model import build_unet\n",
    "# from loss import DiceLoss, DiceBCELoss\n",
    "# from utils import seeding, create_dir, epoch_time\n",
    "\n",
    "'''训练深度学习模型'''\n",
    "def train(model, loader, optimizer, loss_fn, device, show_images=False):\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    model.train()\n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        x = x.to(device, dtype=torch.float32)\n",
    "        y = y.to(device, dtype=torch.float32)\n",
    "\n",
    "        # if i == 1 and show_images:\n",
    "        #    # 显示第一批图像和掩码\n",
    "        #     img = x[0].cpu().numpy()  # 假设图像是CHW格式\n",
    "        #     img = np.transpose(img, (1, 2, 0))  # 转换为HWC格式\n",
    "        #     img = img[..., ::-1]  # 将BGR转换为RGB\n",
    "        #     mask = y[0].cpu().numpy()  # 假设掩码是CHW格式\n",
    "        #     mask = np.transpose(mask, (1, 2, 0))  # 转换为HWC格式\n",
    "\n",
    "        #     plt.figure(figsize=(12, 6))\n",
    "\n",
    "        #     plt.subplot(1, 2, 1)\n",
    "        #     plt.imshow(img)\n",
    "        #     plt.title(\"Sample Image\")\n",
    "\n",
    "        #     plt.subplot(1, 2, 2)\n",
    "        #     plt.imshow(mask, cmap='gray')\n",
    "        #     plt.title(\"Corresponding Mask\")\n",
    "\n",
    "        #     plt.show()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    #计算整个epoch的平均损失\n",
    "    epoch_loss = epoch_loss/len(loader)\n",
    "    return epoch_loss\n",
    "\n",
    "def evaluate(model, loader, loss_fn, device):\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device, dtype=torch.float32)\n",
    "            y = y.to(device, dtype=torch.float32)\n",
    "\n",
    "            y_pred = model(x)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        epoch_loss = epoch_loss/len(loader)\n",
    "    return epoch_loss\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\" Seeding \"\"\"\n",
    "    seeding(42)\n",
    "\n",
    "    \"\"\" Directories \"\"\"\n",
    "    create_dir(\"files\")\n",
    "\n",
    "    \"\"\" Load dataset \"\"\"\n",
    "    train_x = sorted(glob(\"working/new_data/train/image/*\"))\n",
    "    train_y = sorted(glob(\"working/new_data/train/mask/*\"))\n",
    "\n",
    "    valid_x = sorted(glob(\"working/new_data/test/image/*\"))\n",
    "    valid_y = sorted(glob(\"working/new_data/test/mask/*\"))\n",
    "\n",
    "    data_str = f\"Dataset Size:\\nTrain: {len(train_x)} - Valid: {len(valid_x)}\\n\"\n",
    "    print(data_str)\n",
    "\n",
    "    \"\"\" Hyperparameters \"\"\"\n",
    "    H = 560\n",
    "    W = 560\n",
    "    size = (H, W)\n",
    "    batch_size = 64\n",
    "    num_epochs = 100   \n",
    "    lr = 1e-3\n",
    "    checkpoint_path = \"working/files/drive_checkpoint_ori.pth\"\n",
    "\n",
    "    \"\"\" Dataset and loader \"\"\"\n",
    "    train_dataset = DriveDataset(train_x, train_y)\n",
    "    valid_dataset = DriveDataset(valid_x, valid_y)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        dataset=valid_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   ## GTX 1060 6GB\n",
    "    model = build_unet()\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
    "    loss_fn = DiceBCELoss()\n",
    "\n",
    "    \"\"\" Training the model \"\"\"\n",
    "    best_valid_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        #该轮训练的平均损失值 train_loss\n",
    "        train_loss = train(model, train_loader, optimizer, loss_fn, device, show_images=True)\n",
    "        #返回验证损失 valid_loss\n",
    "        valid_loss = evaluate(model, valid_loader, loss_fn, device)\n",
    "\n",
    "        \"\"\" Saving the model \"\"\"\n",
    "        if valid_loss < best_valid_loss:\n",
    "            data_str = f\"Valid loss improved from {best_valid_loss:2.4f} to {valid_loss:2.4f}. Saving checkpoint: {checkpoint_path}\"\n",
    "            print(data_str)\n",
    "\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        data_str = f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\\n'\n",
    "        data_str += f'\\tTrain Loss: {train_loss:.3f}\\n'\n",
    "        data_str += f'\\t Val. Loss: {valid_loss:.3f}\\n'\n",
    "        print(data_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T10:56:04.789919Z",
     "iopub.status.busy": "2025-12-26T10:56:04.789291Z",
     "iopub.status.idle": "2025-12-26T10:56:09.112965Z",
     "shell.execute_reply": "2025-12-26T10:56:09.112196Z",
     "shell.execute_reply.started": "2025-12-26T10:56:04.789886Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1006381/705512160.py:80: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
      "  0%|          | 0/20 [00:00<?, ?it/s][ WARN:0@407.122] global loadsave.cpp:1063 imwrite_ Unsupported depth image for selected encoder is fallbacked to CV_8U.\n",
      " 15%|█▌        | 3/20 [00:00<00:01, 15.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":-- jaccard:0.5623, f1:0.7199,recall:0.7650,precision:0.680,acc:0.9468,specificity:0.965\n",
      ":-- jaccard:0.4266, f1:0.5981,recall:0.4726,precision:0.814,acc:0.9349,specificity:0.988\n",
      ":-- jaccard:0.5486, f1:0.7085,recall:0.6604,precision:0.764,acc:0.9459,specificity:0.977\n",
      ":-- jaccard:0.4865, f1:0.6546,recall:0.5672,precision:0.774,acc:0.9448,specificity:0.983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [00:00<00:00, 18.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":-- jaccard:0.5627, f1:0.7201,recall:0.6718,precision:0.776,acc:0.9512,specificity:0.980\n",
      ":-- jaccard:0.5556, f1:0.7144,recall:0.6465,precision:0.798,acc:0.9496,specificity:0.982\n",
      ":-- jaccard:0.5380, f1:0.6996,recall:0.6439,precision:0.766,acc:0.9494,specificity:0.980\n",
      ":-- jaccard:0.5117, f1:0.6770,recall:0.6160,precision:0.751,acc:0.9494,specificity:0.981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [00:00<00:00, 18.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":-- jaccard:0.4671, f1:0.6368,recall:0.5675,precision:0.725,acc:0.9477,specificity:0.981\n",
      ":-- jaccard:0.5572, f1:0.7156,recall:0.7081,precision:0.723,acc:0.9538,specificity:0.976\n",
      ":-- jaccard:0.4216, f1:0.5931,recall:0.5103,precision:0.708,acc:0.9372,specificity:0.979\n",
      ":-- jaccard:0.5279, f1:0.6910,recall:0.6277,precision:0.769,acc:0.9515,specificity:0.982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [00:00<00:00, 18.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":-- jaccard:0.5557, f1:0.7144,recall:0.6791,precision:0.754,acc:0.9469,specificity:0.976\n",
      ":-- jaccard:0.5607, f1:0.7185,recall:0.7055,precision:0.732,acc:0.9554,specificity:0.977\n",
      ":-- jaccard:0.5242, f1:0.6878,recall:0.7763,precision:0.617,acc:0.9494,specificity:0.963\n",
      ":-- jaccard:0.5764, f1:0.7313,recall:0.7008,precision:0.765,acc:0.9535,specificity:0.979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 18.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":-- jaccard:0.4774, f1:0.6463,recall:0.5633,precision:0.758,acc:0.9479,specificity:0.983\n",
      ":-- jaccard:0.5311, f1:0.6937,recall:0.6466,precision:0.748,acc:0.9548,specificity:0.981\n",
      ":-- jaccard:0.5886, f1:0.7410,recall:0.7864,precision:0.701,acc:0.9546,specificity:0.970\n",
      ":-- jaccard:0.5701, f1:0.7262,recall:0.7635,precision:0.692,acc:0.9576,specificity:0.973\n",
      "\n",
      "Overall---Jaccard: 0.5275 - F1: 0.6894 - Sensitivity: 0.6539 - Precision: 0.7408 - Acc: 0.9491 - Specificity:0.9778\n",
      "FPS:  316.1170316998538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "import os, time\n",
    "from operator import add\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import imageio\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, jaccard_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "# from model import build_unet\n",
    "# from utils import create_dir, seeding\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\" Ground truth \"\"\"\n",
    "    y_true = y_true.cpu().numpy()\n",
    "    y_true = y_true > 0.5\n",
    "    y_true = y_true.astype(np.uint8)\n",
    "    y_true = y_true.reshape(-1)\n",
    "\n",
    "    \"\"\" Prediction \"\"\"\n",
    "    y_pred = y_pred.cpu().numpy()\n",
    "    y_pred = y_pred > 0.5\n",
    "    y_pred = y_pred.astype(np.uint8)\n",
    "    y_pred = y_pred.reshape(-1)\n",
    "\n",
    "    # 使用混淆矩阵计算 TP, FP, FN, TN\n",
    "    # labels=[0, 1] 确保即使数据中缺少某一类也能返回 2x2 矩阵\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "\n",
    "    # Dice Coefficient (F1 Score) = 2 * TP / (2 * TP + FP + FN)\n",
    "    score_f1 = 2 * tp / (2 * tp + fp + fn + 1e-6)\n",
    "\n",
    "    # Jaccard (IoU) = TP / (TP + FP + FN)\n",
    "    score_jaccard = tp / (tp + fp + fn + 1e-6)\n",
    "\n",
    "    # Sensitivity (Recall) = TP / (TP + FN)\n",
    "    score_recall = tp / (tp + fn + 1e-6)\n",
    "\n",
    "    # Specificity = TN / (TN + FP)\n",
    "    score_specificity = tn / (tn + fp + 1e-6)\n",
    "\n",
    "    # Precision = TP / (TP + FP)\n",
    "    score_precision = tp / (tp + fp + 1e-6)\n",
    "\n",
    "    # Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    score_acc = (tp + tn) / (tp + tn + fp + fn + 1e-6)\n",
    "\n",
    "    return [score_jaccard, score_f1, score_recall, score_precision, score_acc, score_specificity]\n",
    "\n",
    "def mask_parse(mask):\n",
    "    mask = np.expand_dims(mask, axis=-1)    ## (512, 512, 1)\n",
    "    mask = np.concatenate([mask, mask, mask], axis=-1)  ## (512, 512, 3)\n",
    "    return mask\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\" Seeding \"\"\"\n",
    "    seeding(42)\n",
    "\n",
    "    \"\"\" Folders \"\"\"\n",
    "    create_dir(\"working/results\")\n",
    "\n",
    "    \"\"\" Load dataset \"\"\"\n",
    "    test_x = sorted(glob(\"working/new_data/test/image/*\"))\n",
    "    test_y = sorted(glob(\"working/new_data/test/mask/*\"))\n",
    "\n",
    "    \"\"\" Hyperparameters \"\"\"\n",
    "    H = 560\n",
    "    W = 560\n",
    "    size = (W, H)\n",
    "    checkpoint_path = \"working/files/drive_checkpoint_ori.pth\"\n",
    "\n",
    "    \"\"\" Load the checkpoint \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model = build_unet()\n",
    "    model = model.to(device)\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    metrics_score = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "    time_taken = []\n",
    "    \n",
    "    # Lists to store metrics for each image for plotting\n",
    "    image_indices = []\n",
    "    jaccard_scores = []\n",
    "    f1_scores = []\n",
    "    recall_scores = []\n",
    "    specificity_scores = []\n",
    "\n",
    "    for i, (x, y) in tqdm(enumerate(zip(test_x, test_y)), total=len(test_x)):\n",
    "        \"\"\" Extract the name \"\"\"\n",
    "        name = x.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "        \"\"\" Reading image \"\"\"\n",
    "        image = cv2.imread(x, cv2.IMREAD_COLOR) ## (512, 512, 3)\n",
    "        ## image = cv2.resize(image, size)\n",
    "        x = np.transpose(image, (2, 0, 1))      ## (3, 512, 512)\n",
    "        x = x/255.0\n",
    "        x = np.expand_dims(x, axis=0)           ## (1, 3, 512, 512)\n",
    "        x = x.astype(np.float32)\n",
    "        x = torch.from_numpy(x)\n",
    "        x = x.to(device)\n",
    "\n",
    "        \"\"\" Reading mask \"\"\"\n",
    "        mask = cv2.imread(y, cv2.IMREAD_GRAYSCALE)  ## (512, 512)\n",
    "        ## mask = cv2.resize(mask, size)\n",
    "        y = np.expand_dims(mask, axis=0)            ## (1, 512, 512)\n",
    "        y = y/255.0\n",
    "        y = np.expand_dims(y, axis=0)               ## (1, 1, 512, 512)\n",
    "        y = y.astype(np.float32)\n",
    "        y = torch.from_numpy(y)\n",
    "        y = y.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \"\"\" Prediction and Calculating FPS \"\"\"\n",
    "            start_time = time.time()\n",
    "            pred_y = model(x)\n",
    "            pred_y = torch.sigmoid(pred_y)\n",
    "            total_time = time.time() - start_time\n",
    "            time_taken.append(total_time)\n",
    "\n",
    "\n",
    "            score = calculate_metrics(y, pred_y)\n",
    "            print(f\":-- jaccard:{score[0]:1.4f}, f1:{score[1]:1.4f},recall:{score[2]:1.4f},precision:{score[3]:1.3f},acc:{score[4]:1.4f},specificity:{score[5]:1.3f}\")\n",
    "            \n",
    "            metrics_score = list(map(add, metrics_score, score))\n",
    "            \n",
    "            # Store for plotting\n",
    "            image_indices.append(i)\n",
    "            jaccard_scores.append(score[0])\n",
    "            f1_scores.append(score[1])\n",
    "            recall_scores.append(score[2])\n",
    "            specificity_scores.append(score[5])\n",
    "            \n",
    "            pred_y = pred_y[0].cpu().numpy()        ## (1, 512, 512)\n",
    "            pred_y = np.squeeze(pred_y, axis=0)     ## (512, 512)\n",
    "            pred_y = pred_y > 0.5\n",
    "            pred_y = np.array(pred_y, dtype=np.uint8)\n",
    "\n",
    "        \"\"\" Saving masks \"\"\"\n",
    "        ori_mask = mask_parse(mask)\n",
    "        pred_y = mask_parse(pred_y)\n",
    "        line = np.ones((size[1], 10, 3)) * 128\n",
    "\n",
    "        cat_images = np.concatenate(\n",
    "            [image, line, ori_mask, line, pred_y * 255], axis=1\n",
    "        )\n",
    "        cv2.imwrite(f\"working/results/{name}.png\", cat_images)\n",
    "\n",
    "    jaccard = metrics_score[0]/len(test_x)\n",
    "    f1 = metrics_score[1]/len(test_x)\n",
    "    recall = metrics_score[2]/len(test_x)\n",
    "    precision = metrics_score[3]/len(test_x)\n",
    "    acc = metrics_score[4]/len(test_x)\n",
    "    spec = metrics_score[5]/len(test_x)\n",
    "    print(f\"\\nOverall---Jaccard: {jaccard:1.4f} - F1: {f1:1.4f} - Sensitivity: {recall:1.4f} - Precision: {precision:1.4f} - Acc: {acc:1.4f} - Specificity:{spec:1.4f}\")\n",
    "\n",
    "    fps = 1/np.mean(time_taken)\n",
    "    print(\"FPS: \", fps)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 946814,
     "sourceId": 1604025,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "pampc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
