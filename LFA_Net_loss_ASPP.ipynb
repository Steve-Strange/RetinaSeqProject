{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T10:17:27.187106Z",
     "iopub.status.busy": "2025-12-26T10:17:27.186304Z",
     "iopub.status.idle": "2025-12-26T10:17:38.866947Z",
     "shell.execute_reply": "2025-12-26T10:17:38.866281Z",
     "shell.execute_reply.started": "2025-12-26T10:17:27.187068Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangziteng/miniconda3/envs/pampc/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 20 - 20\n",
      "Test: 20 - 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  9.10it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 54.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# Data Augmentation\n",
    "import sys\n",
    "import os\n",
    "\n",
    "if os.path.exists(\"config.py\"):\n",
    "    print(\"Warning: Found 'config.py' in the current directory. Please rename it to avoid conflicts with torch.\")\n",
    "if os.path.exists(\"torch.py\"):\n",
    "    print(\"Warning: Found 'torch.py' in the current directory. Please rename it to avoid conflicts with torch.\")\n",
    "\n",
    "# 优先导入 torch\n",
    "import torch\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import imageio\n",
    "from albumentations import HorizontalFlip, VerticalFlip, Rotate, RandomBrightnessContrast, RandomCrop, RandomRotate90, RandomGridShuffle\n",
    "\n",
    "\"\"\" Create a directory \"\"\"\n",
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "'''加载数据：原图+标签'''\n",
    "def load_data(path):\n",
    "    train_x = sorted(glob(os.path.join(path, \"training\", \"images\", \"*.tif\")))\n",
    "    train_y = sorted(glob(os.path.join(path, \"training\", \"1st_manual\", \"*.gif\")))\n",
    "\n",
    "    test_x = sorted(glob(os.path.join(path, \"test\", \"images\", \"*.tif\")))\n",
    "    test_y = sorted(glob(os.path.join(path, \"test\", \"1st_manual\", \"*.gif\")))\n",
    "\n",
    "    return (train_x, train_y), (test_x, test_y)\n",
    "\n",
    "'''\n",
    "增强数据\n",
    "对图像及其对应mask数据增强\n",
    "'''\n",
    "def augment_data(images, masks, save_path, augment=True):\n",
    "    size = (560, 560)\n",
    "\n",
    "    for idx, (x, y) in tqdm(enumerate(zip(images, masks)), total=len(images)):\n",
    "        \"\"\" Extracting the name \"\"\"\n",
    "        name = x.split(os.sep)[-1].split(\".\")[0]\n",
    "        \n",
    "        \"\"\" Reading image and mask \"\"\"\n",
    "        x = cv2.imread(x, cv2.IMREAD_COLOR)\n",
    "        y = imageio.mimread(y)[0]\n",
    "\n",
    "        if x is None or y is None:\n",
    "            print(f\"Error reading image or mask for {name}\")\n",
    "            continue\n",
    "\n",
    "        if augment == True:\n",
    "            transformations = [\n",
    "                HorizontalFlip(p=1.0),\n",
    "                VerticalFlip(p=1.0),\n",
    "                Rotate(limit=45, p=1.0),\n",
    "                RandomBrightnessContrast(p=0.6),\n",
    "                RandomCrop(300, 300, p=0.8),\n",
    "                RandomRotate90(p=1.0),\n",
    "                RandomGridShuffle(p=0.7)\n",
    "            ]\n",
    "\n",
    "            augmented_images = [x]\n",
    "            augmented_masks = [y]\n",
    "\n",
    "            for aug in transformations:\n",
    "                augmented = aug(image=x, mask=y)\n",
    "                augmented_images.append(augmented[\"image\"])\n",
    "                augmented_masks.append(augmented[\"mask\"])\n",
    "            \n",
    "            X = augmented_images\n",
    "            Y = augmented_masks\n",
    "\n",
    "        else:\n",
    "            X = [x]\n",
    "            Y = [y]\n",
    "\n",
    "        index = 0\n",
    "        for i, m in zip(X, Y):\n",
    "            #将图像和掩码调整到目标大小\n",
    "            i = cv2.resize(i, size)\n",
    "            m = cv2.resize(m, size)\n",
    "\n",
    "            tmp_image_name = f\"{name}_{index}.png\"\n",
    "            tmp_mask_name = f\"{name}_{index}.png\"\n",
    "\n",
    "            image_path = os.path.join(save_path, \"image\", tmp_image_name)\n",
    "            mask_path = os.path.join(save_path, \"mask\", tmp_mask_name)\n",
    "\n",
    "            cv2.imwrite(image_path, i)\n",
    "            cv2.imwrite(mask_path, m)\n",
    "\n",
    "            index += 1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\" Seeding \"\"\"\n",
    "    np.random.seed(42)\n",
    "\n",
    "    \"\"\" Load the data \"\"\"\n",
    "    # 修改为相对路径\n",
    "    data_path = \"data/DRIVE/\"\n",
    "    \n",
    "    if os.path.exists(data_path):\n",
    "        (train_x, train_y), (test_x, test_y) = load_data(data_path)\n",
    "\n",
    "        print(f\"Train: {len(train_x)} - {len(train_y)}\")\n",
    "        print(f\"Test: {len(test_x)} - {len(test_y)}\")\n",
    "\n",
    "        \"\"\" Create directories to save the augmented data \"\"\"\n",
    "        create_dir(\"working/new_data/train/image/\")\n",
    "        create_dir(\"working/new_data/train/mask/\")\n",
    "        create_dir(\"working/new_data/test/image/\")\n",
    "        create_dir(\"working/new_data/test/mask/\")\n",
    "\n",
    "        \"\"\" Data augmentation \"\"\"\n",
    "        # 取消注释以运行数据增强\n",
    "        augment_data(train_x, train_y, \"working/new_data/train/\", augment=True)\n",
    "        augment_data(test_x, test_y, \"working/new_data/test/\", augment=False)\n",
    "    else:\n",
    "        print(f\"Data path not found: {data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T10:17:38.868551Z",
     "iopub.status.busy": "2025-12-26T10:17:38.868200Z",
     "iopub.status.idle": "2025-12-26T10:17:40.435461Z",
     "shell.execute_reply": "2025-12-26T10:17:40.434747Z",
     "shell.execute_reply.started": "2025-12-26T10:17:38.868526Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 560, 560])\n"
     ]
    }
   ],
   "source": [
    "# Model (LFA-Net)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalModulation(nn.Module):\n",
    "    def __init__(self, in_channels, gamma=2.0, alpha=0.25):\n",
    "        super(FocalModulation, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.gmp = nn.AdaptiveMaxPool2d(1)\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = self.gap(x)\n",
    "        max_val = self.gmp(x)\n",
    "        modulation = (max_val - mean) * self.alpha\n",
    "        modulation = self.conv(modulation)\n",
    "        modulation = self.sigmoid(modulation)\n",
    "        scaled_inputs = x * modulation\n",
    "        outputs = torch.pow(scaled_inputs, self.gamma)\n",
    "        return outputs\n",
    "\n",
    "class FocalModulationContextAggregation(nn.Module):\n",
    "    def __init__(self, in_channels, filters):\n",
    "        super(FocalModulationContextAggregation, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, filters, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(in_channels, filters, kernel_size=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv_ctx = nn.Conv2d(filters, filters, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.focal_mod = FocalModulation(filters)\n",
    "\n",
    "    def forward(self, x):\n",
    "        c1 = self.relu1(self.conv1(x))\n",
    "        c2 = self.relu2(self.conv2(x))\n",
    "        \n",
    "        global_context = self.gap(c2)\n",
    "        global_context = self.sigmoid(self.conv_ctx(global_context))\n",
    "        global_context = c1 * global_context\n",
    "        \n",
    "        fm = self.focal_mod(global_context)\n",
    "        return torch.cat([c1, fm], dim=1)\n",
    "\n",
    "class VisionMambaInspired(nn.Module):\n",
    "    def __init__(self, dim, dropout_rate=0.1):\n",
    "        super(VisionMambaInspired, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.token_mixer = nn.Sequential(\n",
    "            nn.Conv2d(dim, dim, kernel_size=3, padding=1, groups=dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.channel_mixer = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(dim * 4, dim),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        \n",
    "        shortcut = x\n",
    "        x_perm = x.permute(0, 2, 3, 1)\n",
    "        x_norm = self.norm1(x_perm).permute(0, 3, 1, 2)\n",
    "        x_tm = self.token_mixer(x_norm) + shortcut\n",
    "        \n",
    "        shortcut = x_tm\n",
    "        x_perm = x_tm.permute(0, 2, 3, 1)\n",
    "        x_norm = self.norm2(x_perm)\n",
    "        x_cm = self.channel_mixer(x_norm)\n",
    "        x_cm = x_cm.permute(0, 3, 1, 2)\n",
    "        \n",
    "        return x_cm + shortcut\n",
    "    \n",
    "    \n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ASPP, self).__init__()\n",
    "\n",
    "        self.atrous1 = nn.Conv2d(in_channels, out_channels, 1, padding=0)\n",
    "        self.atrous6 = nn.Conv2d(in_channels, out_channels, 3, padding=6, dilation=6)\n",
    "        self.atrous12 = nn.Conv2d(in_channels, out_channels, 3, padding=12, dilation=12)\n",
    "        self.atrous18 = nn.Conv2d(in_channels, out_channels, 3, padding=18, dilation=18)\n",
    "\n",
    "        self.global_pool = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels, out_channels, 1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.project = nn.Conv2d(out_channels * 5, out_channels, 1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, w = x.shape[2:]\n",
    "\n",
    "        x1 = self.atrous1(x)\n",
    "        x2 = self.atrous6(x)\n",
    "        x3 = self.atrous12(x)\n",
    "        x4 = self.atrous18(x)\n",
    "\n",
    "        x5 = self.global_pool(x)\n",
    "        x5 = F.interpolate(x5, size=(h, w), mode='bilinear', align_corners=True)\n",
    "\n",
    "        x = torch.cat([x1, x2, x3, x4, x5], dim=1)\n",
    "        return self.relu(self.project(x))\n",
    "\n",
    "\n",
    "class LiteFusionAttention(nn.Module):\n",
    "    def __init__(self, in_channels, filters):\n",
    "        super(LiteFusionAttention, self).__init__()\n",
    "        self.proj1 = nn.Conv2d(in_channels, filters, kernel_size=1)\n",
    "        self.norm = nn.LayerNorm(filters)\n",
    "        self.conv = nn.Conv2d(filters, filters, kernel_size=3, padding=1)\n",
    "        self.fmca = FocalModulationContextAggregation(filters, filters)\n",
    "        self.proj2 = nn.Conv2d(2 * filters, filters, kernel_size=1)\n",
    "        self.vm = VisionMambaInspired(filters)\n",
    "        \n",
    "        self.res_proj = nn.Conv2d(in_channels, filters, kernel_size=1) if in_channels != filters else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_tensor = self.proj1(x)\n",
    "        \n",
    "        x_perm = input_tensor.permute(0, 2, 3, 1)\n",
    "        x_norm = self.norm(x_perm).permute(0, 3, 1, 2)\n",
    "        \n",
    "        x_conv = self.conv(x_norm)\n",
    "        x_fmca = self.fmca(x_conv)\n",
    "        x_proj = self.proj2(x_fmca)\n",
    "        \n",
    "        res = self.res_proj(x) if isinstance(self.res_proj, nn.Conv2d) else input_tensor\n",
    "        out = x_proj + res\n",
    "        \n",
    "        out = self.vm(out)\n",
    "        return out\n",
    "\n",
    "class RA_AttentionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, n_classes, k):\n",
    "        super(RA_AttentionBlock, self).__init__()\n",
    "        self.k = k\n",
    "        self.n_classes = n_classes\n",
    "        self.conv = nn.Conv2d(in_channels, k * n_classes, kernel_size=3, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(k * n_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.gmp = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        f = self.relu(self.bn(self.conv(x)))\n",
    "        \n",
    "        x1 = self.gmp(f)\n",
    "        x2 = self.gap(f)\n",
    "        x_mul = x1 * x2\n",
    "        \n",
    "        x_reshape = x_mul.view(b, self.n_classes, self.k)\n",
    "        s = torch.mean(x_reshape, dim=-1, keepdim=False)\n",
    "        \n",
    "        f_perm = f.permute(0, 2, 3, 1)\n",
    "        f_reshape = f_perm.view(b, h, w, self.n_classes, self.k)\n",
    "        f_mean = torch.mean(f_reshape, dim=-1, keepdim=False)\n",
    "        \n",
    "        s_expanded = s.view(b, 1, 1, self.n_classes)\n",
    "        x_weighted = f_mean * s_expanded\n",
    "        \n",
    "        m = torch.mean(x_weighted, dim=-1, keepdim=True)\n",
    "        m = m.permute(0, 3, 1, 2)\n",
    "        \n",
    "        semantic = x * m\n",
    "        return semantic\n",
    "\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout_rate=0.5):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1x1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0, bias=False)\n",
    "        self.conv3x3 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.conv3x3_dilated = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=2, dilation=2, bias=False)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        c1 = self.conv1x1(x)\n",
    "        c3 = self.conv3x3(x)\n",
    "        c3d = self.conv3x3_dilated(x)\n",
    "        out = c1 + c3 + c3d\n",
    "        out = self.leaky_relu(out)\n",
    "        return out\n",
    "\n",
    "class build_unet(nn.Module):\n",
    "    def __init__(self, input_channels=3, num_classes=1, feature_scale=2, dropout=0.5):\n",
    "        super(build_unet, self).__init__()\n",
    "        filters = [int(x / feature_scale) for x in [16, 32, 64]]\n",
    "        \n",
    "        self.conv1 = ConvBlock(input_channels, filters[0], dropout)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.bn1 = nn.BatchNorm2d(filters[0])\n",
    "        \n",
    "        self.conv2 = ConvBlock(filters[0], filters[1], dropout)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.bn2 = nn.BatchNorm2d(filters[1])\n",
    "        \n",
    "        self.conv3 = ConvBlock(filters[1], filters[2], dropout)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.bn3 = nn.BatchNorm2d(filters[2])\n",
    "        \n",
    "        self.lfa = LiteFusionAttention(filters[2], filters=32)\n",
    "        self.aspp = ASPP(32, 32)\n",
    "\n",
    "        \n",
    "        lfa_out_channels = 32 \n",
    "        self.att1 = RA_AttentionBlock(lfa_out_channels, 1, 16)\n",
    "        \n",
    "        self.up1 = nn.ConvTranspose2d(lfa_out_channels * 2, filters[2], kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        \n",
    "        self.att2 = RA_AttentionBlock(filters[1], 1, 16)\n",
    "        self.dec_conv1 = nn.Conv2d(filters[1] + filters[2], filters[2], kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.up2 = nn.ConvTranspose2d(filters[2], filters[2], kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        \n",
    "        self.att3 = RA_AttentionBlock(filters[0], 1, 16)\n",
    "        self.dec_conv2 = nn.Conv2d(filters[0] + filters[2], filters[2], kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.up3 = nn.ConvTranspose2d(filters[2], filters[0], kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.dec_conv3 = nn.Conv2d(filters[0], filters[0], kernel_size=3, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.final = nn.Conv2d(filters[0], num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        c1 = self.conv1(x)\n",
    "        p1 = self.bn1(self.pool1(c1))\n",
    "        \n",
    "        c2 = self.conv2(p1)\n",
    "        p2 = self.bn2(self.pool2(c2))\n",
    "        \n",
    "        c3 = self.conv3(p2)\n",
    "        p3 = self.bn3(self.pool3(c3))\n",
    "        \n",
    "        lfa = self.lfa(p3)\n",
    "        aspp = self.aspp(lfa)\n",
    "        \n",
    "        att1 = self.att1(aspp)\n",
    "        fused = torch.cat([att1, aspp], dim=1)\n",
    "        \n",
    "        d1 = self.up1(fused)\n",
    "        if d1.size() != c2.size():\n",
    "             d1 = F.interpolate(d1, size=c2.shape[2:], mode='bilinear', align_corners=True)\n",
    "             \n",
    "        att2 = self.att2(c2)\n",
    "        d1 = torch.cat([att2, d1], dim=1)\n",
    "        d1 = self.relu1(self.dec_conv1(d1))\n",
    "        \n",
    "        d2 = self.up2(d1)\n",
    "        if d2.size() != c1.size():\n",
    "             d2 = F.interpolate(d2, size=c1.shape[2:], mode='bilinear', align_corners=True)\n",
    "\n",
    "        att3 = self.att3(c1)\n",
    "        d2 = torch.cat([att3, d2], dim=1)\n",
    "        d2 = self.relu2(self.dec_conv2(d2))\n",
    "        \n",
    "        d3 = self.up3(d2)\n",
    "        if d3.size() != x.size():\n",
    "             d3 = F.interpolate(d3, size=x.shape[2:], mode='bilinear', align_corners=True)\n",
    "             \n",
    "        d3 = self.relu3(self.dec_conv3(d3))\n",
    "        \n",
    "        out = self.final(d3)\n",
    "        return out\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    x = torch.randn((2, 3, 560, 560))\n",
    "    f = build_unet()\n",
    "    y = f(x)\n",
    "    print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T10:17:40.437315Z",
     "iopub.status.busy": "2025-12-26T10:17:40.437071Z",
     "iopub.status.idle": "2025-12-26T10:17:40.443522Z",
     "shell.execute_reply": "2025-12-26T10:17:40.442839Z",
     "shell.execute_reply.started": "2025-12-26T10:17:40.437294Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DriveDataset(Dataset):\n",
    "    def __init__(self, images_path, masks_path):\n",
    "\n",
    "        self.images_path = images_path\n",
    "        self.masks_path = masks_path\n",
    "        self.n_samples = len(images_path)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Reading image \"\"\"\n",
    "        image = cv2.imread(self.images_path[index], cv2.IMREAD_COLOR)\n",
    "        image = image/255.0 ## (512, 512, 3)\n",
    "        image = np.transpose(image, (2, 0, 1))  ## (3, 512, 512)\n",
    "        image = image.astype(np.float32)\n",
    "        image = torch.from_numpy(image)\n",
    "\n",
    "        \"\"\" Reading mask \"\"\"\n",
    "        mask = cv2.imread(self.masks_path[index], cv2.IMREAD_GRAYSCALE)\n",
    "        mask = mask/255.0   ## (512, 512)\n",
    "        mask = np.expand_dims(mask, axis=0) ## (1, 512, 512)\n",
    "        mask = mask.astype(np.float32)\n",
    "        mask = torch.from_numpy(mask)\n",
    "        return image, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T10:17:40.445318Z",
     "iopub.status.busy": "2025-12-26T10:17:40.445100Z",
     "iopub.status.idle": "2025-12-26T10:17:40.467440Z",
     "shell.execute_reply": "2025-12-26T10:17:40.466743Z",
     "shell.execute_reply.started": "2025-12-26T10:17:40.445297Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Loss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalTverskyLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal Tversky Loss\n",
    "    特别适合类别极不平衡 + 微结构（血管）\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0.7, beta=0.3, gamma=0.75, smooth=1e-6):\n",
    "        super(FocalTverskyLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        inputs = torch.sigmoid(inputs)\n",
    "\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        TP = (inputs * targets).sum()\n",
    "        FP = ((1 - targets) * inputs).sum()\n",
    "        FN = (targets * (1 - inputs)).sum()\n",
    "\n",
    "        tversky = (TP + self.smooth) / (\n",
    "            TP + self.alpha * FP + self.beta * FN + self.smooth\n",
    "        )\n",
    "\n",
    "        loss = torch.pow((1 - tversky), self.gamma)\n",
    "        return loss\n",
    "\n",
    "class BoundaryLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Boundary Loss via distance transform\n",
    "    强制预测边界贴近 GT 边界\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(BoundaryLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        inputs = torch.sigmoid(inputs)\n",
    "\n",
    "        # 计算边界（简单 Sobel）\n",
    "        def get_boundary(x):\n",
    "            sobel_x = torch.tensor([[1, 0, -1],\n",
    "                                    [2, 0, -2],\n",
    "                                    [1, 0, -1]], device=x.device).float()\n",
    "            sobel_y = sobel_x.t()\n",
    "\n",
    "            sobel_x = sobel_x.view(1, 1, 3, 3)\n",
    "            sobel_y = sobel_y.view(1, 1, 3, 3)\n",
    "\n",
    "            grad_x = F.conv2d(x, sobel_x, padding=1)\n",
    "            grad_y = F.conv2d(x, sobel_y, padding=1)\n",
    "\n",
    "            return torch.sqrt(grad_x ** 2 + grad_y ** 2 + 1e-6)\n",
    "\n",
    "        pred_boundary = get_boundary(inputs)\n",
    "        gt_boundary = get_boundary(targets)\n",
    "\n",
    "        return F.l1_loss(pred_boundary, gt_boundary)\n",
    "\n",
    "class VesselSegmentationLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    最终联合损失：\n",
    "    Focal Tversky + Boundary\n",
    "    \"\"\"\n",
    "    def __init__(self, w_tversky=1.0, w_boundary=0.1):\n",
    "        super(VesselSegmentationLoss, self).__init__()\n",
    "        self.tversky = FocalTverskyLoss()\n",
    "        self.boundary = BoundaryLoss()\n",
    "        self.w_tversky = w_tversky\n",
    "        self.w_boundary = w_boundary\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        loss_t = self.tversky(inputs, targets)\n",
    "        loss_b = self.boundary(inputs, targets)\n",
    "        return self.w_tversky * loss_t + self.w_boundary * loss_b\n",
    "\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "\n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = torch.sigmoid(inputs)\n",
    "\n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n",
    "\n",
    "        return 1 - dice\n",
    "\n",
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "\n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = torch.sigmoid(inputs)\n",
    "\n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n",
    "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
    "        Dice_BCE = BCE + dice_loss\n",
    "\n",
    "        return Dice_BCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T10:17:40.468545Z",
     "iopub.status.busy": "2025-12-26T10:17:40.468339Z",
     "iopub.status.idle": "2025-12-26T10:17:40.482989Z",
     "shell.execute_reply": "2025-12-26T10:17:40.482370Z",
     "shell.execute_reply.started": "2025-12-26T10:17:40.468525Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Utils\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "\"\"\" Seeding the randomness. \"\"\"\n",
    "def seeding(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\"\"\" Create a directory. \"\"\"\n",
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "\"\"\" Calculate the time taken \"\"\"\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T10:17:40.483887Z",
     "iopub.status.busy": "2025-12-26T10:17:40.483664Z",
     "iopub.status.idle": "2025-12-26T10:56:04.787586Z",
     "shell.execute_reply": "2025-12-26T10:56:04.786752Z",
     "shell.execute_reply.started": "2025-12-26T10:17:40.483865Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size:\n",
      "Train: 160 - Valid: 20\n",
      "\n",
      "Valid loss improved from inf to 0.9501. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 01 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.947\n",
      "\t Val. Loss: 0.950\n",
      "\n",
      "Valid loss improved from 0.9501 to 0.9500. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 02 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.945\n",
      "\t Val. Loss: 0.950\n",
      "\n",
      "Valid loss improved from 0.9500 to 0.9491. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 03 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.944\n",
      "\t Val. Loss: 0.949\n",
      "\n",
      "Valid loss improved from 0.9491 to 0.9416. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 04 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.929\n",
      "\t Val. Loss: 0.942\n",
      "\n",
      "Valid loss improved from 0.9416 to 0.9141. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 05 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.908\n",
      "\t Val. Loss: 0.914\n",
      "\n",
      "Valid loss improved from 0.9141 to 0.8941. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 06 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.896\n",
      "\t Val. Loss: 0.894\n",
      "\n",
      "Valid loss improved from 0.8941 to 0.8899. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 07 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.910\n",
      "\t Val. Loss: 0.890\n",
      "\n",
      "Epoch: 08 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.872\n",
      "\t Val. Loss: 0.896\n",
      "\n",
      "Epoch: 09 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.884\n",
      "\t Val. Loss: 0.892\n",
      "\n",
      "Valid loss improved from 0.8899 to 0.8808. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 10 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.853\n",
      "\t Val. Loss: 0.881\n",
      "\n",
      "Epoch: 11 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.814\n",
      "\t Val. Loss: 0.895\n",
      "\n",
      "Valid loss improved from 0.8808 to 0.8717. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 12 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.798\n",
      "\t Val. Loss: 0.872\n",
      "\n",
      "Valid loss improved from 0.8717 to 0.8617. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 13 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.773\n",
      "\t Val. Loss: 0.862\n",
      "\n",
      "Valid loss improved from 0.8617 to 0.8342. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 14 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.758\n",
      "\t Val. Loss: 0.834\n",
      "\n",
      "Valid loss improved from 0.8342 to 0.8270. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 15 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.739\n",
      "\t Val. Loss: 0.827\n",
      "\n",
      "Valid loss improved from 0.8270 to 0.8141. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 16 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.720\n",
      "\t Val. Loss: 0.814\n",
      "\n",
      "Epoch: 17 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.691\n",
      "\t Val. Loss: 0.838\n",
      "\n",
      "Epoch: 18 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.653\n",
      "\t Val. Loss: 0.863\n",
      "\n",
      "Valid loss improved from 0.8141 to 0.8059. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 19 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.627\n",
      "\t Val. Loss: 0.806\n",
      "\n",
      "Valid loss improved from 0.8059 to 0.6550. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 20 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.613\n",
      "\t Val. Loss: 0.655\n",
      "\n",
      "Valid loss improved from 0.6550 to 0.6502. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 21 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.596\n",
      "\t Val. Loss: 0.650\n",
      "\n",
      "Valid loss improved from 0.6502 to 0.6415. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 22 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.578\n",
      "\t Val. Loss: 0.642\n",
      "\n",
      "Epoch: 23 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.568\n",
      "\t Val. Loss: 0.793\n",
      "\n",
      "Epoch: 24 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.551\n",
      "\t Val. Loss: 0.708\n",
      "\n",
      "Epoch: 25 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.537\n",
      "\t Val. Loss: 0.678\n",
      "\n",
      "Valid loss improved from 0.6415 to 0.6272. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 26 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.521\n",
      "\t Val. Loss: 0.627\n",
      "\n",
      "Valid loss improved from 0.6272 to 0.5976. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 27 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.500\n",
      "\t Val. Loss: 0.598\n",
      "\n",
      "Valid loss improved from 0.5976 to 0.5935. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 28 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.489\n",
      "\t Val. Loss: 0.594\n",
      "\n",
      "Epoch: 29 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.472\n",
      "\t Val. Loss: 0.670\n",
      "\n",
      "Valid loss improved from 0.5935 to 0.4710. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 30 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.466\n",
      "\t Val. Loss: 0.471\n",
      "\n",
      "Epoch: 31 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.454\n",
      "\t Val. Loss: 0.479\n",
      "\n",
      "Epoch: 32 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.439\n",
      "\t Val. Loss: 0.496\n",
      "\n",
      "Epoch: 33 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.433\n",
      "\t Val. Loss: 0.483\n",
      "\n",
      "Epoch: 34 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.426\n",
      "\t Val. Loss: 0.581\n",
      "\n",
      "Valid loss improved from 0.4710 to 0.4397. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 35 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.421\n",
      "\t Val. Loss: 0.440\n",
      "\n",
      "Valid loss improved from 0.4397 to 0.4362. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 36 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.416\n",
      "\t Val. Loss: 0.436\n",
      "\n",
      "Valid loss improved from 0.4362 to 0.4106. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 37 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.409\n",
      "\t Val. Loss: 0.411\n",
      "\n",
      "Valid loss improved from 0.4106 to 0.4018. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 38 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.406\n",
      "\t Val. Loss: 0.402\n",
      "\n",
      "Valid loss improved from 0.4018 to 0.4018. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 39 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.401\n",
      "\t Val. Loss: 0.402\n",
      "\n",
      "Valid loss improved from 0.4018 to 0.3895. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 40 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.399\n",
      "\t Val. Loss: 0.390\n",
      "\n",
      "Epoch: 41 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.397\n",
      "\t Val. Loss: 0.399\n",
      "\n",
      "Epoch: 42 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.395\n",
      "\t Val. Loss: 0.392\n",
      "\n",
      "Valid loss improved from 0.3895 to 0.3854. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 43 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.392\n",
      "\t Val. Loss: 0.385\n",
      "\n",
      "Valid loss improved from 0.3854 to 0.3757. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 44 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.393\n",
      "\t Val. Loss: 0.376\n",
      "\n",
      "Epoch: 45 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.387\n",
      "\t Val. Loss: 0.377\n",
      "\n",
      "Epoch: 46 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.390\n",
      "\t Val. Loss: 0.383\n",
      "\n",
      "Epoch: 47 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.389\n",
      "\t Val. Loss: 0.392\n",
      "\n",
      "Epoch: 48 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.389\n",
      "\t Val. Loss: 0.380\n",
      "\n",
      "Epoch: 49 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.385\n",
      "\t Val. Loss: 0.379\n",
      "\n",
      "Valid loss improved from 0.3757 to 0.3686. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 50 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.382\n",
      "\t Val. Loss: 0.369\n",
      "\n",
      "Epoch: 51 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.379\n",
      "\t Val. Loss: 0.372\n",
      "\n",
      "Valid loss improved from 0.3686 to 0.3651. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 52 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.378\n",
      "\t Val. Loss: 0.365\n",
      "\n",
      "Valid loss improved from 0.3651 to 0.3636. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 53 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.378\n",
      "\t Val. Loss: 0.364\n",
      "\n",
      "Epoch: 54 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.377\n",
      "\t Val. Loss: 0.370\n",
      "\n",
      "Epoch: 55 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.376\n",
      "\t Val. Loss: 0.369\n",
      "\n",
      "Epoch: 56 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.375\n",
      "\t Val. Loss: 0.367\n",
      "\n",
      "Epoch: 57 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.371\n",
      "\t Val. Loss: 0.364\n",
      "\n",
      "Valid loss improved from 0.3636 to 0.3607. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 58 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.373\n",
      "\t Val. Loss: 0.361\n",
      "\n",
      "Epoch: 59 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.374\n",
      "\t Val. Loss: 0.370\n",
      "\n",
      "Epoch: 60 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.373\n",
      "\t Val. Loss: 0.364\n",
      "\n",
      "Epoch: 61 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.370\n",
      "\t Val. Loss: 0.361\n",
      "\n",
      "Valid loss improved from 0.3607 to 0.3570. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 62 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.368\n",
      "\t Val. Loss: 0.357\n",
      "\n",
      "Epoch: 63 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.369\n",
      "\t Val. Loss: 0.359\n",
      "\n",
      "Valid loss improved from 0.3570 to 0.3567. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 64 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.367\n",
      "\t Val. Loss: 0.357\n",
      "\n",
      "Valid loss improved from 0.3567 to 0.3557. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 65 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.367\n",
      "\t Val. Loss: 0.356\n",
      "\n",
      "Epoch: 66 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.366\n",
      "\t Val. Loss: 0.359\n",
      "\n",
      "Valid loss improved from 0.3557 to 0.3546. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 67 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.367\n",
      "\t Val. Loss: 0.355\n",
      "\n",
      "Epoch: 68 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.366\n",
      "\t Val. Loss: 0.355\n",
      "\n",
      "Valid loss improved from 0.3546 to 0.3533. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 69 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.363\n",
      "\t Val. Loss: 0.353\n",
      "\n",
      "Valid loss improved from 0.3533 to 0.3529. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 70 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.364\n",
      "\t Val. Loss: 0.353\n",
      "\n",
      "Valid loss improved from 0.3529 to 0.3524. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 71 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.362\n",
      "\t Val. Loss: 0.352\n",
      "\n",
      "Epoch: 72 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.359\n",
      "\t Val. Loss: 0.358\n",
      "\n",
      "Valid loss improved from 0.3524 to 0.3516. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 73 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.359\n",
      "\t Val. Loss: 0.352\n",
      "\n",
      "Valid loss improved from 0.3516 to 0.3507. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 74 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.360\n",
      "\t Val. Loss: 0.351\n",
      "\n",
      "Epoch: 75 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.360\n",
      "\t Val. Loss: 0.360\n",
      "\n",
      "Epoch: 76 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.362\n",
      "\t Val. Loss: 0.353\n",
      "\n",
      "Valid loss improved from 0.3507 to 0.3503. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 77 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.358\n",
      "\t Val. Loss: 0.350\n",
      "\n",
      "Valid loss improved from 0.3503 to 0.3502. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 78 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.358\n",
      "\t Val. Loss: 0.350\n",
      "\n",
      "Epoch: 79 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.358\n",
      "\t Val. Loss: 0.362\n",
      "\n",
      "Epoch: 80 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.359\n",
      "\t Val. Loss: 0.352\n",
      "\n",
      "Valid loss improved from 0.3502 to 0.3496. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 81 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.358\n",
      "\t Val. Loss: 0.350\n",
      "\n",
      "Epoch: 82 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.358\n",
      "\t Val. Loss: 0.358\n",
      "\n",
      "Epoch: 83 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.359\n",
      "\t Val. Loss: 0.351\n",
      "\n",
      "Epoch: 84 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.356\n",
      "\t Val. Loss: 0.375\n",
      "\n",
      "Epoch: 85 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.356\n",
      "\t Val. Loss: 0.358\n",
      "\n",
      "Epoch: 86 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.355\n",
      "\t Val. Loss: 0.353\n",
      "\n",
      "Epoch: 87 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.354\n",
      "\t Val. Loss: 0.352\n",
      "\n",
      "Valid loss improved from 0.3496 to 0.3488. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 88 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.355\n",
      "\t Val. Loss: 0.349\n",
      "\n",
      "Epoch: 89 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.353\n",
      "\t Val. Loss: 0.355\n",
      "\n",
      "Epoch: 90 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.353\n",
      "\t Val. Loss: 0.349\n",
      "\n",
      "Valid loss improved from 0.3488 to 0.3463. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 91 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.354\n",
      "\t Val. Loss: 0.346\n",
      "\n",
      "Epoch: 92 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.351\n",
      "\t Val. Loss: 0.348\n",
      "\n",
      "Epoch: 93 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.350\n",
      "\t Val. Loss: 0.347\n",
      "\n",
      "Epoch: 94 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.349\n",
      "\t Val. Loss: 0.347\n",
      "\n",
      "Epoch: 95 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.347\n",
      "\t Val. Loss: 0.348\n",
      "\n",
      "Valid loss improved from 0.3463 to 0.3451. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 96 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.348\n",
      "\t Val. Loss: 0.345\n",
      "\n",
      "Epoch: 97 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.348\n",
      "\t Val. Loss: 0.356\n",
      "\n",
      "Epoch: 98 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.352\n",
      "\t Val. Loss: 0.348\n",
      "\n",
      "Valid loss improved from 0.3451 to 0.3448. Saving checkpoint: working/files/drive_checkpoint_loss_ASPP_CBAM.pth\n",
      "Epoch: 99 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.349\n",
      "\t Val. Loss: 0.345\n",
      "\n",
      "Epoch: 100 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.348\n",
      "\t Val. Loss: 0.360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "import os\n",
    "import time\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "# from data import DriveDataset\n",
    "# from model import build_unet\n",
    "# from loss import DiceLoss, DiceBCELoss\n",
    "# from utils import seeding, create_dir, epoch_time\n",
    "\n",
    "'''训练深度学习模型'''\n",
    "def train(model, loader, optimizer, loss_fn, device, show_images=False):\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    model.train()\n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        x = x.to(device, dtype=torch.float32)\n",
    "        y = y.to(device, dtype=torch.float32)\n",
    "\n",
    "        # if i == 1 and show_images:\n",
    "        #    # 显示第一批图像和掩码\n",
    "        #     img = x[0].cpu().numpy()  # 假设图像是CHW格式\n",
    "        #     img = np.transpose(img, (1, 2, 0))  # 转换为HWC格式\n",
    "        #     img = img[..., ::-1]  # 将BGR转换为RGB\n",
    "        #     mask = y[0].cpu().numpy()  # 假设掩码是CHW格式\n",
    "        #     mask = np.transpose(mask, (1, 2, 0))  # 转换为HWC格式\n",
    "\n",
    "        #     plt.figure(figsize=(12, 6))\n",
    "\n",
    "        #     plt.subplot(1, 2, 1)\n",
    "        #     plt.imshow(img)\n",
    "        #     plt.title(\"Sample Image\")\n",
    "\n",
    "        #     plt.subplot(1, 2, 2)\n",
    "        #     plt.imshow(mask, cmap='gray')\n",
    "        #     plt.title(\"Corresponding Mask\")\n",
    "\n",
    "        #     plt.show()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    #计算整个epoch的平均损失\n",
    "    epoch_loss = epoch_loss/len(loader)\n",
    "    return epoch_loss\n",
    "\n",
    "def evaluate(model, loader, loss_fn, device):\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device, dtype=torch.float32)\n",
    "            y = y.to(device, dtype=torch.float32)\n",
    "\n",
    "            y_pred = model(x)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        epoch_loss = epoch_loss/len(loader)\n",
    "    return epoch_loss\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\" Seeding \"\"\"\n",
    "    seeding(42)\n",
    "\n",
    "    \"\"\" Directories \"\"\"\n",
    "    create_dir(\"files\")\n",
    "\n",
    "    \"\"\" Load dataset \"\"\"\n",
    "    train_x = sorted(glob(\"working/new_data/train/image/*\"))\n",
    "    train_y = sorted(glob(\"working/new_data/train/mask/*\"))\n",
    "\n",
    "    valid_x = sorted(glob(\"working/new_data/test/image/*\"))\n",
    "    valid_y = sorted(glob(\"working/new_data/test/mask/*\"))\n",
    "\n",
    "    data_str = f\"Dataset Size:\\nTrain: {len(train_x)} - Valid: {len(valid_x)}\\n\"\n",
    "    print(data_str)\n",
    "\n",
    "    \"\"\" Hyperparameters \"\"\"\n",
    "    H = 560\n",
    "    W = 560\n",
    "    size = (H, W)\n",
    "    batch_size = 64\n",
    "    num_epochs = 100   \n",
    "    lr = 1e-3\n",
    "    checkpoint_path = \"working/files/drive_checkpoint_loss_ASPP.pth\"\n",
    "\n",
    "    \"\"\" Dataset and loader \"\"\"\n",
    "    train_dataset = DriveDataset(train_x, train_y)\n",
    "    valid_dataset = DriveDataset(valid_x, valid_y)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        dataset=valid_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   ## GTX 1060 6GB\n",
    "    model = build_unet()\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
    "    # loss_fn = DiceBCELoss()\n",
    "    \n",
    "    loss_fn = VesselSegmentationLoss(\n",
    "        w_tversky=1.0,\n",
    "        w_boundary=0.1\n",
    "    )\n",
    "\n",
    "\n",
    "    \"\"\" Training the model \"\"\"\n",
    "    best_valid_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        #该轮训练的平均损失值 train_loss\n",
    "        train_loss = train(model, train_loader, optimizer, loss_fn, device, show_images=True)\n",
    "        #返回验证损失 valid_loss\n",
    "        valid_loss = evaluate(model, valid_loader, loss_fn, device)\n",
    "\n",
    "        \"\"\" Saving the model \"\"\"\n",
    "        if valid_loss < best_valid_loss:\n",
    "            data_str = f\"Valid loss improved from {best_valid_loss:2.4f} to {valid_loss:2.4f}. Saving checkpoint: {checkpoint_path}\"\n",
    "            print(data_str)\n",
    "\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        data_str = f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\\n'\n",
    "        data_str += f'\\tTrain Loss: {train_loss:.3f}\\n'\n",
    "        data_str += f'\\t Val. Loss: {valid_loss:.3f}\\n'\n",
    "        print(data_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T10:56:04.789919Z",
     "iopub.status.busy": "2025-12-26T10:56:04.789291Z",
     "iopub.status.idle": "2025-12-26T10:56:09.112965Z",
     "shell.execute_reply": "2025-12-26T10:56:09.112196Z",
     "shell.execute_reply.started": "2025-12-26T10:56:04.789886Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_793603/3563123304.py:80: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@532.770] global loadsave.cpp:1063 imwrite_ Unsupported depth image for selected encoder is fallbacked to CV_8U.\n",
      " 15%|█▌        | 3/20 [00:00<00:01, 14.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":-- jaccard:0.6538, f1:0.7907,recall:0.7626,precision:0.821,acc:0.9641,specificity:0.984\n",
      ":-- jaccard:0.6736, f1:0.8050,recall:0.7284,precision:0.899,acc:0.9640,specificity:0.991\n",
      ":-- jaccard:0.5423, f1:0.7032,recall:0.5731,precision:0.910,acc:0.9518,specificity:0.994\n",
      ":-- jaccard:0.6350, f1:0.7768,recall:0.6883,precision:0.891,acc:0.9637,specificity:0.992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [00:00<00:00, 17.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":-- jaccard:0.6216, f1:0.7667,recall:0.6635,precision:0.908,acc:0.9623,specificity:0.993\n",
      ":-- jaccard:0.6012, f1:0.7509,recall:0.6534,precision:0.883,acc:0.9579,specificity:0.991\n",
      ":-- jaccard:0.5748, f1:0.7300,recall:0.6156,precision:0.897,acc:0.9583,specificity:0.993\n",
      ":-- jaccard:0.5569, f1:0.7154,recall:0.5976,precision:0.891,acc:0.9594,specificity:0.993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [00:00<00:00, 17.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":-- jaccard:0.5888, f1:0.7412,recall:0.6477,precision:0.866,acc:0.9633,specificity:0.991\n",
      ":-- jaccard:0.6183, f1:0.7641,recall:0.7004,precision:0.841,acc:0.9648,specificity:0.988\n",
      ":-- jaccard:0.6121, f1:0.7593,recall:0.6886,precision:0.846,acc:0.9613,specificity:0.988\n",
      ":-- jaccard:0.5726, f1:0.7283,recall:0.6055,precision:0.914,acc:0.9612,specificity:0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [00:00<00:00, 18.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":-- jaccard:0.5943, f1:0.7456,recall:0.6337,precision:0.905,acc:0.9577,specificity:0.993\n",
      ":-- jaccard:0.6091, f1:0.7571,recall:0.6693,precision:0.871,acc:0.9653,specificity:0.991\n",
      ":-- jaccard:0.6207, f1:0.7659,recall:0.6971,precision:0.850,acc:0.9696,specificity:0.991\n",
      ":-- jaccard:0.6174, f1:0.7635,recall:0.6918,precision:0.852,acc:0.9613,specificity:0.988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 17.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":-- jaccard:0.5896, f1:0.7418,recall:0.6574,precision:0.851,acc:0.9614,specificity:0.989\n",
      ":-- jaccard:0.6333, f1:0.7755,recall:0.7520,precision:0.801,acc:0.9655,specificity:0.984\n",
      ":-- jaccard:0.6931, f1:0.8187,recall:0.8097,precision:0.828,acc:0.9702,specificity:0.985\n",
      ":-- jaccard:0.6319, f1:0.7744,recall:0.7449,precision:0.806,acc:0.9680,specificity:0.986\n",
      "\n",
      "Overall---Jaccard: 0.6120 - F1: 0.7587 - Sensitivity: 0.6790 - Precision: 0.8665 - Acc: 0.9626 - Specificity:0.9899\n",
      "FPS:  287.8094035627041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "import os, time\n",
    "from operator import add\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import imageio\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, jaccard_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "# from model import build_unet\n",
    "# from utils import create_dir, seeding\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\" Ground truth \"\"\"\n",
    "    y_true = y_true.cpu().numpy()\n",
    "    y_true = y_true > 0.5\n",
    "    y_true = y_true.astype(np.uint8)\n",
    "    y_true = y_true.reshape(-1)\n",
    "\n",
    "    \"\"\" Prediction \"\"\"\n",
    "    y_pred = y_pred.cpu().numpy()\n",
    "    y_pred = y_pred > 0.5\n",
    "    y_pred = y_pred.astype(np.uint8)\n",
    "    y_pred = y_pred.reshape(-1)\n",
    "\n",
    "    # 使用混淆矩阵计算 TP, FP, FN, TN\n",
    "    # labels=[0, 1] 确保即使数据中缺少某一类也能返回 2x2 矩阵\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "\n",
    "    # Dice Coefficient (F1 Score) = 2 * TP / (2 * TP + FP + FN)\n",
    "    score_f1 = 2 * tp / (2 * tp + fp + fn + 1e-6)\n",
    "\n",
    "    # Jaccard (IoU) = TP / (TP + FP + FN)\n",
    "    score_jaccard = tp / (tp + fp + fn + 1e-6)\n",
    "\n",
    "    # Sensitivity (Recall) = TP / (TP + FN)\n",
    "    score_recall = tp / (tp + fn + 1e-6)\n",
    "\n",
    "    # Specificity = TN / (TN + FP)\n",
    "    score_specificity = tn / (tn + fp + 1e-6)\n",
    "\n",
    "    # Precision = TP / (TP + FP)\n",
    "    score_precision = tp / (tp + fp + 1e-6)\n",
    "\n",
    "    # Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    score_acc = (tp + tn) / (tp + tn + fp + fn + 1e-6)\n",
    "\n",
    "    return [score_jaccard, score_f1, score_recall, score_precision, score_acc, score_specificity]\n",
    "\n",
    "def mask_parse(mask):\n",
    "    mask = np.expand_dims(mask, axis=-1)    ## (512, 512, 1)\n",
    "    mask = np.concatenate([mask, mask, mask], axis=-1)  ## (512, 512, 3)\n",
    "    return mask\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\" Seeding \"\"\"\n",
    "    seeding(42)\n",
    "\n",
    "    \"\"\" Folders \"\"\"\n",
    "    create_dir(\"working/results\")\n",
    "\n",
    "    \"\"\" Load dataset \"\"\"\n",
    "    test_x = sorted(glob(\"working/new_data/test/image/*\"))\n",
    "    test_y = sorted(glob(\"working/new_data/test/mask/*\"))\n",
    "\n",
    "    \"\"\" Hyperparameters \"\"\"\n",
    "    H = 560\n",
    "    W = 560\n",
    "    size = (W, H)\n",
    "    checkpoint_path = \"working/files/drive_checkpoint_loss_ASPP.pth\"\n",
    "\n",
    "    \"\"\" Load the checkpoint \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model = build_unet()\n",
    "    model = model.to(device)\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    metrics_score = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "    time_taken = []\n",
    "    \n",
    "    # Lists to store metrics for each image for plotting\n",
    "    image_indices = []\n",
    "    jaccard_scores = []\n",
    "    f1_scores = []\n",
    "    recall_scores = []\n",
    "    specificity_scores = []\n",
    "\n",
    "    for i, (x, y) in tqdm(enumerate(zip(test_x, test_y)), total=len(test_x)):\n",
    "        \"\"\" Extract the name \"\"\"\n",
    "        name = x.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "        \"\"\" Reading image \"\"\"\n",
    "        image = cv2.imread(x, cv2.IMREAD_COLOR) ## (512, 512, 3)\n",
    "        ## image = cv2.resize(image, size)\n",
    "        x = np.transpose(image, (2, 0, 1))      ## (3, 512, 512)\n",
    "        x = x/255.0\n",
    "        x = np.expand_dims(x, axis=0)           ## (1, 3, 512, 512)\n",
    "        x = x.astype(np.float32)\n",
    "        x = torch.from_numpy(x)\n",
    "        x = x.to(device)\n",
    "\n",
    "        \"\"\" Reading mask \"\"\"\n",
    "        mask = cv2.imread(y, cv2.IMREAD_GRAYSCALE)  ## (512, 512)\n",
    "        ## mask = cv2.resize(mask, size)\n",
    "        y = np.expand_dims(mask, axis=0)            ## (1, 512, 512)\n",
    "        y = y/255.0\n",
    "        y = np.expand_dims(y, axis=0)               ## (1, 1, 512, 512)\n",
    "        y = y.astype(np.float32)\n",
    "        y = torch.from_numpy(y)\n",
    "        y = y.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \"\"\" Prediction and Calculating FPS \"\"\"\n",
    "            start_time = time.time()\n",
    "            pred_y = model(x)\n",
    "            pred_y = torch.sigmoid(pred_y)\n",
    "            total_time = time.time() - start_time\n",
    "            time_taken.append(total_time)\n",
    "\n",
    "\n",
    "            score = calculate_metrics(y, pred_y)\n",
    "            print(f\":-- jaccard:{score[0]:1.4f}, f1:{score[1]:1.4f},recall:{score[2]:1.4f},precision:{score[3]:1.3f},acc:{score[4]:1.4f},specificity:{score[5]:1.3f}\")\n",
    "            \n",
    "            metrics_score = list(map(add, metrics_score, score))\n",
    "            \n",
    "            # Store for plotting\n",
    "            image_indices.append(i)\n",
    "            jaccard_scores.append(score[0])\n",
    "            f1_scores.append(score[1])\n",
    "            recall_scores.append(score[2])\n",
    "            specificity_scores.append(score[5])\n",
    "            \n",
    "            pred_y = pred_y[0].cpu().numpy()        ## (1, 512, 512)\n",
    "            pred_y = np.squeeze(pred_y, axis=0)     ## (512, 512)\n",
    "            pred_y = pred_y > 0.5\n",
    "            pred_y = np.array(pred_y, dtype=np.uint8)\n",
    "\n",
    "        \"\"\" Saving masks \"\"\"\n",
    "        ori_mask = mask_parse(mask)\n",
    "        pred_y = mask_parse(pred_y)\n",
    "        line = np.ones((size[1], 10, 3)) * 128\n",
    "\n",
    "        cat_images = np.concatenate(\n",
    "            [image, line, ori_mask, line, pred_y * 255], axis=1\n",
    "        )\n",
    "        cv2.imwrite(f\"working/results/{name}.png\", cat_images)\n",
    "\n",
    "    jaccard = metrics_score[0]/len(test_x)\n",
    "    f1 = metrics_score[1]/len(test_x)\n",
    "    recall = metrics_score[2]/len(test_x)\n",
    "    precision = metrics_score[3]/len(test_x)\n",
    "    acc = metrics_score[4]/len(test_x)\n",
    "    spec = metrics_score[5]/len(test_x)\n",
    "    print(f\"\\nOverall---Jaccard: {jaccard:1.4f} - F1: {f1:1.4f} - Sensitivity: {recall:1.4f} - Precision: {precision:1.4f} - Acc: {acc:1.4f} - Specificity:{spec:1.4f}\")\n",
    "\n",
    "    fps = 1/np.mean(time_taken)\n",
    "    print(\"FPS: \", fps)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 946814,
     "sourceId": 1604025,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "pampc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
